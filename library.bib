
@inproceedings{deacNeuralAlgorithmicReasoners2021,
	title = {Neural {Algorithmic} {Reasoners} are {Implicit} {Planners}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/82e9e7a12665240d13d0b928be28f230-Abstract.html},
	urldate = {2024-01-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 34: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2021, {NeurIPS} 2021, {December} 6-14, 2021, virtual},
	author = {Deac, Andreea and Velickovic, Petar and Milinkovic, Ognjen and Bacon, Pierre-Luc and Tang, Jian and Nikolic, Mladen},
	editor = {Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann N. and Liang, Percy and Vaughan, Jennifer Wortman},
	year = {2021},
	pages = {15529--15542},
}

@inproceedings{numerosoDualAlgorithmicReasoning2023,
	title = {Dual {Algorithmic} {Reasoning}},
	url = {https://openreview.net/pdf?id=hhvkdRdWt1F},
	urldate = {2024-01-23},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}, {ICLR} 2023, {Kigali}, {Rwanda}, {May} 1-5, 2023},
	publisher = {OpenReview.net},
	author = {Numeroso, Danilo and Bacciu, Davide and Velickovic, Petar},
	year = {2023},
	keywords = {notion},
}

@inproceedings{mengLocatingEditingROME2022,
	title = {Locating and {Editing} {Factual} {Associations} in {GPT}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 35: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2022, {NeurIPS} 2022, {New} {Orleans}, {LA}, {USA}, {November} 28 - {December} 9, 2022},
	author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
	editor = {Koyejo, Sanmi and Mohamed, S. and Agarwal, A. and Belgrave, Danielle and Cho, K. and Oh, A.},
	year = {2022},
	keywords = {notion},
}

@article{huChatGPTSetsRecord2023,
	chapter = {Technology},
	title = {{ChatGPT} sets record for fastest-growing user base - analyst note},
	url = {https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/},
	abstract = {ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after launch, making it the fastest-growing consumer application in history, according to a UBS study on Wednesday.},
	language = {en},
	urldate = {2024-01-14},
	journal = {Reuters},
	author = {Hu, Krystal},
	month = feb,
	year = {2023},
}

@inproceedings{juerssRecursiveAlgorithmicReasoning2023Openreview,
	title = {Recursive {Algorithmic} {Reasoning}},
	url = {https://openreview.net/forum?id=43M1bPorxU},
	booktitle = {The {Second} {Learning} on {Graphs} {Conference}},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja and Veličković, Petar},
	year = {2023},
}

@inproceedings{juerssRecursiveReasoningNeural2023,
	title = {Recursive {Reasoning} with {Neural} {Networks}},
	url = {https://openreview.net/pdf?id=TS8l4VS7_BK},
	booktitle = {The {First} {Tiny} {Papers} {Track} at {ICLR} 2023, {Tiny} {Papers} @ {ICLR} 2023, {Kigali}, {Rwanda}, {May} 5, 2023},
	publisher = {OpenReview.net},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja},
	editor = {Maughan, Krystal and Liu, Rosanne and Burns, Thomas F.},
	year = {2023},
}

@inproceedings{zintgrafVariBADVeryGood2020,
	title = {{VariBAD}: {A} {Very} {Good} {Method} for {Bayes}-{Adaptive} {Deep} {RL} via {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=Hkl9JlBYvr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Zintgraf, Luisa M. and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
	year = {2020},
}

@misc{guMambaLinearTimeSequence2023,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
}

@inproceedings{yasunagaQAGNNReasoningLanguage2021,
	title = {{QA}-{GNN}: {Reasoning} with {Language} {Models} and {Knowledge} {Graphs} for {Question} {Answering}},
	url = {https://doi.org/10.18653/v1/2021.naacl-main.45},
	doi = {10.18653/V1/2021.NAACL-MAIN.45},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2021, {Online}, {June} 6-11, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tür, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	year = {2021},
	keywords = {notion},
	pages = {535--546},
}

@inproceedings{mengGNNLMLanguageModeling2021,
	title = {{GNN}-{LM}: {Language} {Modeling} based on {Global} {Contexts} via {GNN}},
	shorttitle = {{GNN}-{LM}},
	url = {https://openreview.net/forum?id=BS49l-B5Bql},
	abstract = {Inspired by the notion that "it to copy is easier than to memorize", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.},
	language = {en},
	urldate = {2023-12-02},
	author = {Meng, Yuxian and Zong, Shi and Li, Xiaoya and Sun, Xiaofei and Zhang, Tianwei and Wu, Fei and Li, Jiwei},
	month = oct,
	year = {2021},
}

@misc{juerssHELP2023,
	title = {Everybody {Needs} a {Little} {HELP}: {Explaining} {Graphs} via {Hierarchical} {Concepts}},
	shorttitle = {Everybody {Needs} a {Little} {HELP}},
	url = {http://arxiv.org/abs/2311.15112},
	doi = {10.48550/arXiv.2311.15112},
	abstract = {Graph neural networks (GNNs) have led to major breakthroughs in a variety of domains such as drug discovery, social network analysis, and travel time estimation. However, they lack interpretability which hinders human trust and thereby deployment to settings with high-stakes decisions. A line of interpretable methods approach this by discovering a small set of relevant concepts as subgraphs in the last GNN layer that together explain the prediction. This can yield oversimplified explanations, failing to explain the interaction between GNN layers. To address this oversight, we provide HELP (Hierarchical Explainable Latent Pooling), a novel, inherently interpretable graph pooling approach that reveals how concepts from different GNN layers compose to new ones in later steps. HELP is more than 1-WL expressive and is the first non-spectral, end-to-end-learnable, hierarchical graph pooling method that can learn to pool a variable number of arbitrary connected components. We empirically demonstrate that it performs on-par with standard GCNs and popular pooling methods in terms of accuracy while yielding explanations that are aligned with expert knowledge in the domains of chemistry and social networks. In addition to a qualitative analysis, we employ concept completeness scores as well as concept conformity, a novel metric to measure the noise in discovered concepts, quantitatively verifying that the discovered concepts are significantly easier to fully understand than those from previous work. Our work represents a first step towards an understanding of graph neural networks that goes beyond a set of concepts from the final layer and instead explains the complex interplay of concepts on different levels.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Jürß, Jonas and Magister, Lucie Charlotte and Barbiero, Pietro and Liò, Pietro and Simidjievski, Nikola},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lewisOneBiggestProblems2023,
	title = {One of the {Biggest} {Problems} in {Biology} {Has} {Finally} {Been} {Solved}},
	url = {https://www.scientificamerican.com/article/one-of-the-biggest-problems-in-biology-has-finally-been-solved/},
	abstract = {Google DeepMind CEO Demis Hassabis explains how its AlphaFold AI program predicted the 3-D structure of every known protein},
	language = {en},
	urldate = {2023-11-27},
	journal = {Scientific American},
	author = {Lewis, Tanya},
	month = feb,
	year = {2023},
}

@article{jumperAlphaFold2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2023-11-24},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
}

@misc{schutBridgingHumanAIKnowledge2023,
	title = {Bridging the {Human}-{AI} {Knowledge} {Gap}: {Concept} {Discovery} and {Transfer} in {AlphaZero}},
	shorttitle = {Bridging the {Human}-{AI} {Knowledge} {Gap}},
	url = {http://arxiv.org/abs/2310.16410},
	abstract = {Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Schut, Lisa and Tomasev, Nenad and McGrath, Tom and Hassabis, Demis and Paquet, Ulrich and Kim, Been},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16410 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@misc{holtgenDeDUCEGeneratingCounterfactual2021,
	title = {{DeDUCE}: {Generating} {Counterfactual} {Explanations} {Efficiently}},
	shorttitle = {{DeDUCE}},
	url = {http://arxiv.org/abs/2111.15639},
	doi = {10.48550/arXiv.2111.15639},
	abstract = {When an image classifier outputs a wrong class label, it can be helpful to see what changes in the image would lead to a correct classification. This is the aim of algorithms generating counterfactual explanations. However, there is no easily scalable method to generate such counterfactuals. We develop a new algorithm providing counterfactual explanations for large image classifiers trained with spectral normalisation at low computational cost. We empirically compare this algorithm against baselines from the literature; our novel algorithm consistently finds counterfactuals that are much closer to the original inputs. At the same time, the realism of these counterfactuals is comparable to the baselines. The code for all experiments is available at https://github.com/benedikthoeltgen/DeDUCE.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Höltgen, Benedikt and Schut, Lisa and Brauner, Jan M. and Gal, Yarin},
	month = nov,
	year = {2021},
	note = {arXiv:2111.15639 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@inproceedings{kanBrainNetworkTransformer2022,
	title = {Brain {Network} {Transformer}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/a408234a9b80604a9cf6ca518e474550-Abstract-Conference.html},
	booktitle = {{NeurIPS}},
	author = {Kan, Xuan and Dai, Wei and Cui, Hejie and Zhang, Zilong and Guo, Ying and Yang, Carl},
	year = {2022},
}

@inproceedings{nakkiranDeepDoubleDescent2020,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	url = {https://openreview.net/forum?id=B1g5sA4twr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	year = {2020},
}

@misc{powerGrokkingGeneralizationOverfitting2022,
	title = {Grokking: {Generalization} {Beyond} {Overfitting} on {Small} {Algorithmic} {Datasets}},
	shorttitle = {Grokking},
	url = {http://arxiv.org/abs/2201.02177},
	doi = {10.48550/arXiv.2201.02177},
	abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
	urldate = {2023-11-12},
	publisher = {arXiv},
	author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02177 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{footeNeuronGraphInterpreting2023,
	title = {Neuron to {Graph}: {Interpreting} {Language} {Model} {Neurons} at {Scale}},
	shorttitle = {Neuron to {Graph}},
	url = {http://arxiv.org/abs/2305.19911},
	doi = {10.48550/arXiv.2305.19911},
	abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
	month = may,
	year = {2023},
	note = {arXiv:2305.19911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{giovanniOverSquashingMessagePassing2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}: {The} {Impact} of {Width}, {Depth}, and {Topology}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/di-giovanni23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Giovanni, Francesco Di and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio, Pietro and Bronstein, Michael M.},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {7865--7885},
}

@article{finkelshteinSinglenodeAttacksFooling2022,
	title = {Single-node attacks for fooling graph neural networks},
	volume = {513},
	url = {https://doi.org/10.1016/j.neucom.2022.09.115},
	doi = {10.1016/J.NEUCOM.2022.09.115},
	journal = {Neurocomputing},
	author = {Finkelshtein, Ben and Baskin, Chaim and Zheltonozhskii, Evgenii and Alon, Uri},
	year = {2022},
	keywords = {notion},
	pages = {1--12},
}

@misc{lanLocatingCrossTaskSequence2023,
	title = {Locating {Cross}-{Task} {Sequence} {Continuation} {Circuits} in {Transformers}},
	url = {http://arxiv.org/abs/2311.04131},
	doi = {10.48550/arXiv.2311.04131},
	abstract = {While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Lan, Michael and Barez, Fazl},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{gevaTransformerFeedForwardLayers2022,
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {https://doi.org/10.18653/v1/2022.emnlp-main.3},
	doi = {10.18653/V1/2022.EMNLP-MAIN.3},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2022, {Abu} {Dhabi}, {United} {Arab} {Emirates}, {December} 7-11, 2022},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	year = {2022},
	keywords = {notion},
	pages = {30--45},
}

@inproceedings{gevaTransformerFeedForwardLayers2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {https://doi.org/10.18653/v1/2021.emnlp-main.446},
	doi = {10.18653/V1/2021.EMNLP-MAIN.446},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2021, {Virtual} {Event} / {Punta} {Cana}, {Dominican} {Republic}, 7-11 {November}, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	year = {2021},
	keywords = {notion},
	pages = {5484--5495},
}

@inproceedings{chughtaiToyModelUniversality2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Toy} {Model} of {Universality}: {Reverse} {Engineering} how {Networks} {Learn} {Group} {Operations}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/chughtai23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {6243--6267},
}

@inproceedings{krizhevskyAlexNet2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
	year = {2012},
}

@article{olahNaturallyOccurringEquivariance2020,
	title = {Naturally {Occurring} {Equivariance} in {Neural} {Networks}},
	url = {https://distill.pub/2020/circuits/equivariance},
	doi = {10.23915/distill.00024.004},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Voss, Chelsea and Schubert, Ludwig and Goh, Gabriel},
	year = {2020},
	keywords = {notion},
}

@article{cammarataCurveDetectors2020,
	title = {Curve {Detectors}},
	url = {https://distill.pub/2020/circuits/curve-detectors},
	doi = {10.23915/distill.00024.003},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
	year = {2020},
}

@article{olahZoomIntroductionCircuits2020,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	url = {https://distill.pub/2020/circuits/zoom-in},
	doi = {10.23915/distill.00024.001},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
	year = {2020},
	keywords = {notion},
}

@article{petrovWeightBanding2021,
	title = {Weight {Banding}},
	url = {https://distill.pub/2020/circuits/weight-banding},
	doi = {10.23915/distill.00024.009},
	journal = {Distill},
	author = {Petrov, Michael and Voss, Chelsea and Schubert, Ludwig and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{yangHowGNNsLearn2023,
	title = {How {Graph} {Neural} {Networks} {Learn}: {Lessons} from {Training} {Dynamics} in {Function} {Space}},
	shorttitle = {How {Graph} {Neural} {Networks} {Learn}},
	url = {http://arxiv.org/abs/2310.05105},
	abstract = {A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, however it remains less clear whether and how GNNs learn desired functions during the optimization process. To fill this critical gap, we study the learning dynamics of GNNs in function space via the analytic framework of overparameterization. In particular, we find that the seemingly complicated training process of GNNs can be re-cast into a more familiar label propagation framework, due to the graph inductive bias implicit in this process. From this vantage point, we provide explanations for why the learned GNN functions successfully generalize and for their pathological behavior on heterophilic graphs, which are consistent with observations. Practically, sparsifying and implementing the learning dynamics lead to a minimalist semi-supervised learning algorithm with the efficiency of classic algorithms and the effectiveness of modern GNNs.},
	urldate = {2023-10-30},
	publisher = {arXiv},
	author = {Yang, Chenxiao and Wu, Qitian and Wipf, David and Sun, Ruoyu and Yan, Junchi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05105 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
}

@misc{kipfVariationalGraphAutoEncoders2016,
	title = {Variational {Graph} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1611.07308},
	doi = {10.48550/arXiv.1611.07308},
	abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = nov,
	year = {2016},
	note = {arXiv:1611.07308 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@article{olahMechanisticInterpretabilityVariables2022,
	title = {Mechanistic {Interpretability}, {Variables}, and the {Importance} of {Interpretable} {Bases}},
	url = {https://transformer-circuits.pub/2022/mech-interp-essay/},
	journal = {Transformer Circuits Thread},
	author = {Olah, Chris},
	year = {2022},
}

@article{olahFeatureVisualization2017,
	title = {Feature {Visualization}},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	year = {2017},
}

@article{johnsonExtensionsLipschitzMappings1984,
	title = {Extensions of {Lipschitz} mappings into {Hilbert} space},
	volume = {26},
	url = {https://stanford.edu/class/cs114/readings/JL-Johnson.pdf},
	journal = {Contemporary mathematics},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	year = {1984},
	pages = {189--206},
}

@article{aroraLinearAlgebraicStructure2018,
	title = {Linear {Algebraic} {Structure} of {Word} {Senses}, with {Applications} to {Polysemy}},
	volume = {6},
	url = {https://doi.org/10.1162/tacl_a_00034},
	doi = {10.1162/tacl\_a\_00034},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	year = {2018},
	keywords = {notion},
	pages = {483--495},
}

@article{heimersheimCircuitPythonDocstrings2023,
	title = {A circuit for {Python} docstrings in a 4-layer attention-only transformer},
	url = {https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only},
	author = {Heimersheim, Stefan and Janiak, Jett},
	year = {2023},
	keywords = {notion},
}

@article{schubertHighLowFrequencyDetectors2021,
	title = {High-{Low} {Frequency} {Detectors}},
	url = {https://distill.pub/2020/circuits/frequency-edges},
	doi = {10.23915/distill.00024.005},
	journal = {Distill},
	author = {Schubert, Ludwig and Voss, Chelsea and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{cammarataCurveCircuits2021,
	title = {Curve {Circuits}},
	url = {https://distill.pub/2020/circuits/curve-circuits},
	doi = {10.23915/distill.00024.006},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Voss, Chelsea and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}


@inproceedings{numerosoDualAlgorithmicReasoning2023,
	title = {Dual {Algorithmic} {Reasoning}},
	url = {https://openreview.net/pdf?id=hhvkdRdWt1F},
	urldate = {2024-01-23},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}, {ICLR} 2023, {Kigali}, {Rwanda}, {May} 1-5, 2023},
	publisher = {OpenReview.net},
	author = {Numeroso, Danilo and Bacciu, Davide and Velickovic, Petar},
	year = {2023},
	keywords = {notion},
}

@inproceedings{mengLocatingEditingROME2022,
	title = {Locating and {Editing} {Factual} {Associations} in {GPT}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 35: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2022, {NeurIPS} 2022, {New} {Orleans}, {LA}, {USA}, {November} 28 - {December} 9, 2022},
	author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
	editor = {Koyejo, Sanmi and Mohamed, S. and Agarwal, A. and Belgrave, Danielle and Cho, K. and Oh, A.},
	year = {2022},
	keywords = {notion},
}

@article{huChatGPTSetsRecord2023,
	chapter = {Technology},
	title = {{ChatGPT} sets record for fastest-growing user base - analyst note},
	url = {https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/},
	abstract = {ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after launch, making it the fastest-growing consumer application in history, according to a UBS study on Wednesday.},
	language = {en},
	urldate = {2024-01-14},
	journal = {Reuters},
	author = {Hu, Krystal},
	month = feb,
	year = {2023},
}

@inproceedings{juerssRecursiveAlgorithmicReasoning2023Openreview,
	title = {Recursive {Algorithmic} {Reasoning}},
	url = {https://openreview.net/forum?id=43M1bPorxU},
	booktitle = {The {Second} {Learning} on {Graphs} {Conference}},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja and Veličković, Petar},
	year = {2023},
}

@inproceedings{juerssRecursiveReasoningNeural2023,
	title = {Recursive {Reasoning} with {Neural} {Networks}},
	url = {https://openreview.net/pdf?id=TS8l4VS7_BK},
	booktitle = {The {First} {Tiny} {Papers} {Track} at {ICLR} 2023, {Tiny} {Papers} @ {ICLR} 2023, {Kigali}, {Rwanda}, {May} 5, 2023},
	publisher = {OpenReview.net},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja},
	editor = {Maughan, Krystal and Liu, Rosanne and Burns, Thomas F.},
	year = {2023},
}

@inproceedings{zintgrafVariBADVeryGood2020,
	title = {{VariBAD}: {A} {Very} {Good} {Method} for {Bayes}-{Adaptive} {Deep} {RL} via {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=Hkl9JlBYvr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Zintgraf, Luisa M. and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
	year = {2020},
}

@misc{guMambaLinearTimeSequence2023,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
}

@inproceedings{yasunagaQAGNNReasoningLanguage2021,
	title = {{QA}-{GNN}: {Reasoning} with {Language} {Models} and {Knowledge} {Graphs} for {Question} {Answering}},
	url = {https://doi.org/10.18653/v1/2021.naacl-main.45},
	doi = {10.18653/V1/2021.NAACL-MAIN.45},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2021, {Online}, {June} 6-11, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tür, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	year = {2021},
	keywords = {notion},
	pages = {535--546},
}

@inproceedings{mengGNNLMLanguageModeling2021,
	title = {{GNN}-{LM}: {Language} {Modeling} based on {Global} {Contexts} via {GNN}},
	shorttitle = {{GNN}-{LM}},
	url = {https://openreview.net/forum?id=BS49l-B5Bql},
	abstract = {Inspired by the notion that "it to copy is easier than to memorize", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.},
	language = {en},
	urldate = {2023-12-02},
	author = {Meng, Yuxian and Zong, Shi and Li, Xiaoya and Sun, Xiaofei and Zhang, Tianwei and Wu, Fei and Li, Jiwei},
	month = oct,
	year = {2021},
}

@misc{juerssHELP2023,
	title = {Everybody {Needs} a {Little} {HELP}: {Explaining} {Graphs} via {Hierarchical} {Concepts}},
	shorttitle = {Everybody {Needs} a {Little} {HELP}},
	url = {http://arxiv.org/abs/2311.15112},
	doi = {10.48550/arXiv.2311.15112},
	abstract = {Graph neural networks (GNNs) have led to major breakthroughs in a variety of domains such as drug discovery, social network analysis, and travel time estimation. However, they lack interpretability which hinders human trust and thereby deployment to settings with high-stakes decisions. A line of interpretable methods approach this by discovering a small set of relevant concepts as subgraphs in the last GNN layer that together explain the prediction. This can yield oversimplified explanations, failing to explain the interaction between GNN layers. To address this oversight, we provide HELP (Hierarchical Explainable Latent Pooling), a novel, inherently interpretable graph pooling approach that reveals how concepts from different GNN layers compose to new ones in later steps. HELP is more than 1-WL expressive and is the first non-spectral, end-to-end-learnable, hierarchical graph pooling method that can learn to pool a variable number of arbitrary connected components. We empirically demonstrate that it performs on-par with standard GCNs and popular pooling methods in terms of accuracy while yielding explanations that are aligned with expert knowledge in the domains of chemistry and social networks. In addition to a qualitative analysis, we employ concept completeness scores as well as concept conformity, a novel metric to measure the noise in discovered concepts, quantitatively verifying that the discovered concepts are significantly easier to fully understand than those from previous work. Our work represents a first step towards an understanding of graph neural networks that goes beyond a set of concepts from the final layer and instead explains the complex interplay of concepts on different levels.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Jürß, Jonas and Magister, Lucie Charlotte and Barbiero, Pietro and Liò, Pietro and Simidjievski, Nikola},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lewisOneBiggestProblems2023,
	title = {One of the {Biggest} {Problems} in {Biology} {Has} {Finally} {Been} {Solved}},
	url = {https://www.scientificamerican.com/article/one-of-the-biggest-problems-in-biology-has-finally-been-solved/},
	abstract = {Google DeepMind CEO Demis Hassabis explains how its AlphaFold AI program predicted the 3-D structure of every known protein},
	language = {en},
	urldate = {2023-11-27},
	journal = {Scientific American},
	author = {Lewis, Tanya},
	month = feb,
	year = {2023},
}

@article{jumperAlphaFold2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2023-11-24},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
}

@misc{schutBridgingHumanAIKnowledge2023,
	title = {Bridging the {Human}-{AI} {Knowledge} {Gap}: {Concept} {Discovery} and {Transfer} in {AlphaZero}},
	shorttitle = {Bridging the {Human}-{AI} {Knowledge} {Gap}},
	url = {http://arxiv.org/abs/2310.16410},
	abstract = {Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Schut, Lisa and Tomasev, Nenad and McGrath, Tom and Hassabis, Demis and Paquet, Ulrich and Kim, Been},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16410 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@misc{holtgenDeDUCEGeneratingCounterfactual2021,
	title = {{DeDUCE}: {Generating} {Counterfactual} {Explanations} {Efficiently}},
	shorttitle = {{DeDUCE}},
	url = {http://arxiv.org/abs/2111.15639},
	doi = {10.48550/arXiv.2111.15639},
	abstract = {When an image classifier outputs a wrong class label, it can be helpful to see what changes in the image would lead to a correct classification. This is the aim of algorithms generating counterfactual explanations. However, there is no easily scalable method to generate such counterfactuals. We develop a new algorithm providing counterfactual explanations for large image classifiers trained with spectral normalisation at low computational cost. We empirically compare this algorithm against baselines from the literature; our novel algorithm consistently finds counterfactuals that are much closer to the original inputs. At the same time, the realism of these counterfactuals is comparable to the baselines. The code for all experiments is available at https://github.com/benedikthoeltgen/DeDUCE.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Höltgen, Benedikt and Schut, Lisa and Brauner, Jan M. and Gal, Yarin},
	month = nov,
	year = {2021},
	note = {arXiv:2111.15639 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@inproceedings{kanBrainNetworkTransformer2022,
	title = {Brain {Network} {Transformer}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/a408234a9b80604a9cf6ca518e474550-Abstract-Conference.html},
	booktitle = {{NeurIPS}},
	author = {Kan, Xuan and Dai, Wei and Cui, Hejie and Zhang, Zilong and Guo, Ying and Yang, Carl},
	year = {2022},
}

@inproceedings{nakkiranDeepDoubleDescent2020,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	url = {https://openreview.net/forum?id=B1g5sA4twr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	year = {2020},
}

@misc{powerGrokkingGeneralizationOverfitting2022,
	title = {Grokking: {Generalization} {Beyond} {Overfitting} on {Small} {Algorithmic} {Datasets}},
	shorttitle = {Grokking},
	url = {http://arxiv.org/abs/2201.02177},
	doi = {10.48550/arXiv.2201.02177},
	abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
	urldate = {2023-11-12},
	publisher = {arXiv},
	author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02177 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{footeNeuronGraphInterpreting2023,
	title = {Neuron to {Graph}: {Interpreting} {Language} {Model} {Neurons} at {Scale}},
	shorttitle = {Neuron to {Graph}},
	url = {http://arxiv.org/abs/2305.19911},
	doi = {10.48550/arXiv.2305.19911},
	abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
	month = may,
	year = {2023},
	note = {arXiv:2305.19911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{giovanniOverSquashingMessagePassing2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}: {The} {Impact} of {Width}, {Depth}, and {Topology}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/di-giovanni23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Giovanni, Francesco Di and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio, Pietro and Bronstein, Michael M.},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {7865--7885},
}

@article{finkelshteinSinglenodeAttacksFooling2022,
	title = {Single-node attacks for fooling graph neural networks},
	volume = {513},
	url = {https://doi.org/10.1016/j.neucom.2022.09.115},
	doi = {10.1016/J.NEUCOM.2022.09.115},
	journal = {Neurocomputing},
	author = {Finkelshtein, Ben and Baskin, Chaim and Zheltonozhskii, Evgenii and Alon, Uri},
	year = {2022},
	keywords = {notion},
	pages = {1--12},
}

@misc{lanLocatingCrossTaskSequence2023,
	title = {Locating {Cross}-{Task} {Sequence} {Continuation} {Circuits} in {Transformers}},
	url = {http://arxiv.org/abs/2311.04131},
	doi = {10.48550/arXiv.2311.04131},
	abstract = {While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Lan, Michael and Barez, Fazl},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{gevaTransformerFeedForwardLayers2022,
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {https://doi.org/10.18653/v1/2022.emnlp-main.3},
	doi = {10.18653/V1/2022.EMNLP-MAIN.3},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2022, {Abu} {Dhabi}, {United} {Arab} {Emirates}, {December} 7-11, 2022},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	year = {2022},
	keywords = {notion},
	pages = {30--45},
}

@inproceedings{gevaTransformerFeedForwardLayers2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {https://doi.org/10.18653/v1/2021.emnlp-main.446},
	doi = {10.18653/V1/2021.EMNLP-MAIN.446},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2021, {Virtual} {Event} / {Punta} {Cana}, {Dominican} {Republic}, 7-11 {November}, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	year = {2021},
	keywords = {notion},
	pages = {5484--5495},
}

@inproceedings{chughtaiToyModelUniversality2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Toy} {Model} of {Universality}: {Reverse} {Engineering} how {Networks} {Learn} {Group} {Operations}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/chughtai23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {6243--6267},
}

@inproceedings{krizhevskyAlexNet2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
	year = {2012},
}

@article{olahNaturallyOccurringEquivariance2020,
	title = {Naturally {Occurring} {Equivariance} in {Neural} {Networks}},
	url = {https://distill.pub/2020/circuits/equivariance},
	doi = {10.23915/distill.00024.004},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Voss, Chelsea and Schubert, Ludwig and Goh, Gabriel},
	year = {2020},
	keywords = {notion},
}

@article{cammarataCurveDetectors2020,
	title = {Curve {Detectors}},
	url = {https://distill.pub/2020/circuits/curve-detectors},
	doi = {10.23915/distill.00024.003},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
	year = {2020},
}

@article{olahZoomIntroductionCircuits2020,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	url = {https://distill.pub/2020/circuits/zoom-in},
	doi = {10.23915/distill.00024.001},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
	year = {2020},
	keywords = {notion},
}

@article{petrovWeightBanding2021,
	title = {Weight {Banding}},
	url = {https://distill.pub/2020/circuits/weight-banding},
	doi = {10.23915/distill.00024.009},
	journal = {Distill},
	author = {Petrov, Michael and Voss, Chelsea and Schubert, Ludwig and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{yangHowGNNsLearn2023,
	title = {How {Graph} {Neural} {Networks} {Learn}: {Lessons} from {Training} {Dynamics} in {Function} {Space}},
	shorttitle = {How {Graph} {Neural} {Networks} {Learn}},
	url = {http://arxiv.org/abs/2310.05105},
	abstract = {A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, however it remains less clear whether and how GNNs learn desired functions during the optimization process. To fill this critical gap, we study the learning dynamics of GNNs in function space via the analytic framework of overparameterization. In particular, we find that the seemingly complicated training process of GNNs can be re-cast into a more familiar label propagation framework, due to the graph inductive bias implicit in this process. From this vantage point, we provide explanations for why the learned GNN functions successfully generalize and for their pathological behavior on heterophilic graphs, which are consistent with observations. Practically, sparsifying and implementing the learning dynamics lead to a minimalist semi-supervised learning algorithm with the efficiency of classic algorithms and the effectiveness of modern GNNs.},
	urldate = {2023-10-30},
	publisher = {arXiv},
	author = {Yang, Chenxiao and Wu, Qitian and Wipf, David and Sun, Ruoyu and Yan, Junchi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05105 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
}

@misc{kipfVariationalGraphAutoEncoders2016,
	title = {Variational {Graph} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1611.07308},
	doi = {10.48550/arXiv.1611.07308},
	abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = nov,
	year = {2016},
	note = {arXiv:1611.07308 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@article{olahMechanisticInterpretabilityVariables2022,
	title = {Mechanistic {Interpretability}, {Variables}, and the {Importance} of {Interpretable} {Bases}},
	url = {https://transformer-circuits.pub/2022/mech-interp-essay/},
	journal = {Transformer Circuits Thread},
	author = {Olah, Chris},
	year = {2022},
}

@article{olahFeatureVisualization2017,
	title = {Feature {Visualization}},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	year = {2017},
}

@article{johnsonExtensionsLipschitzMappings1984,
	title = {Extensions of {Lipschitz} mappings into {Hilbert} space},
	volume = {26},
	url = {https://stanford.edu/class/cs114/readings/JL-Johnson.pdf},
	journal = {Contemporary mathematics},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	year = {1984},
	pages = {189--206},
}

@article{aroraLinearAlgebraicStructure2018,
	title = {Linear {Algebraic} {Structure} of {Word} {Senses}, with {Applications} to {Polysemy}},
	volume = {6},
	url = {https://doi.org/10.1162/tacl_a_00034},
	doi = {10.1162/tacl\_a\_00034},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	year = {2018},
	keywords = {notion},
	pages = {483--495},
}

@article{heimersheimCircuitPythonDocstrings2023,
	title = {A circuit for {Python} docstrings in a 4-layer attention-only transformer},
	url = {https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only},
	author = {Heimersheim, Stefan and Janiak, Jett},
	year = {2023},
	keywords = {notion},
}

@article{schubertHighLowFrequencyDetectors2021,
	title = {High-{Low} {Frequency} {Detectors}},
	url = {https://distill.pub/2020/circuits/frequency-edges},
	doi = {10.23915/distill.00024.005},
	journal = {Distill},
	author = {Schubert, Ludwig and Voss, Chelsea and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{cammarataCurveCircuits2021,
	title = {Curve {Circuits}},
	url = {https://distill.pub/2020/circuits/curve-circuits},
	doi = {10.23915/distill.00024.006},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Voss, Chelsea and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{vossBranchSpecialization2021,
	title = {Branch {Specialization}},
	url = {https://distill.pub/2020/circuits/branch-specialization},
	doi = {10.23915/distill.00024.008},
	journal = {Distill},
	author = {Voss, Chelsea and Goh, Gabriel and Cammarata, Nick and Petrov, Michael and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}


@inproceedings{numerosoDualAlgorithmicReasoning2023,
	title = {Dual {Algorithmic} {Reasoning}},
	url = {https://openreview.net/pdf?id=hhvkdRdWt1F},
	urldate = {2024-01-23},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}, {ICLR} 2023, {Kigali}, {Rwanda}, {May} 1-5, 2023},
	publisher = {OpenReview.net},
	author = {Numeroso, Danilo and Bacciu, Davide and Velickovic, Petar},
	year = {2023},
	keywords = {notion},
}

@inproceedings{mengLocatingEditingROME2022,
	title = {Locating and {Editing} {Factual} {Associations} in {GPT}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 35: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2022, {NeurIPS} 2022, {New} {Orleans}, {LA}, {USA}, {November} 28 - {December} 9, 2022},
	author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
	editor = {Koyejo, Sanmi and Mohamed, S. and Agarwal, A. and Belgrave, Danielle and Cho, K. and Oh, A.},
	year = {2022},
	keywords = {notion},
}

@article{huChatGPTSetsRecord2023,
	chapter = {Technology},
	title = {{ChatGPT} sets record for fastest-growing user base - analyst note},
	url = {https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/},
	abstract = {ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after launch, making it the fastest-growing consumer application in history, according to a UBS study on Wednesday.},
	language = {en},
	urldate = {2024-01-14},
	journal = {Reuters},
	author = {Hu, Krystal},
	month = feb,
	year = {2023},
}

@inproceedings{juerssRecursiveAlgorithmicReasoning2023Openreview,
	title = {Recursive {Algorithmic} {Reasoning}},
	url = {https://openreview.net/forum?id=43M1bPorxU},
	booktitle = {The {Second} {Learning} on {Graphs} {Conference}},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja and Veličković, Petar},
	year = {2023},
}

@inproceedings{juerssRecursiveReasoningNeural2023,
	title = {Recursive {Reasoning} with {Neural} {Networks}},
	url = {https://openreview.net/pdf?id=TS8l4VS7_BK},
	booktitle = {The {First} {Tiny} {Papers} {Track} at {ICLR} 2023, {Tiny} {Papers} @ {ICLR} 2023, {Kigali}, {Rwanda}, {May} 5, 2023},
	publisher = {OpenReview.net},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja},
	editor = {Maughan, Krystal and Liu, Rosanne and Burns, Thomas F.},
	year = {2023},
}

@inproceedings{zintgrafVariBADVeryGood2020,
	title = {{VariBAD}: {A} {Very} {Good} {Method} for {Bayes}-{Adaptive} {Deep} {RL} via {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=Hkl9JlBYvr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Zintgraf, Luisa M. and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
	year = {2020},
}

@misc{guMambaLinearTimeSequence2023,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
}

@inproceedings{yasunagaQAGNNReasoningLanguage2021,
	title = {{QA}-{GNN}: {Reasoning} with {Language} {Models} and {Knowledge} {Graphs} for {Question} {Answering}},
	url = {https://doi.org/10.18653/v1/2021.naacl-main.45},
	doi = {10.18653/V1/2021.NAACL-MAIN.45},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2021, {Online}, {June} 6-11, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tür, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	year = {2021},
	keywords = {notion},
	pages = {535--546},
}

@inproceedings{mengGNNLMLanguageModeling2021,
	title = {{GNN}-{LM}: {Language} {Modeling} based on {Global} {Contexts} via {GNN}},
	shorttitle = {{GNN}-{LM}},
	url = {https://openreview.net/forum?id=BS49l-B5Bql},
	abstract = {Inspired by the notion that "it to copy is easier than to memorize", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.},
	language = {en},
	urldate = {2023-12-02},
	author = {Meng, Yuxian and Zong, Shi and Li, Xiaoya and Sun, Xiaofei and Zhang, Tianwei and Wu, Fei and Li, Jiwei},
	month = oct,
	year = {2021},
}

@misc{juerssHELP2023,
	title = {Everybody {Needs} a {Little} {HELP}: {Explaining} {Graphs} via {Hierarchical} {Concepts}},
	shorttitle = {Everybody {Needs} a {Little} {HELP}},
	url = {http://arxiv.org/abs/2311.15112},
	doi = {10.48550/arXiv.2311.15112},
	abstract = {Graph neural networks (GNNs) have led to major breakthroughs in a variety of domains such as drug discovery, social network analysis, and travel time estimation. However, they lack interpretability which hinders human trust and thereby deployment to settings with high-stakes decisions. A line of interpretable methods approach this by discovering a small set of relevant concepts as subgraphs in the last GNN layer that together explain the prediction. This can yield oversimplified explanations, failing to explain the interaction between GNN layers. To address this oversight, we provide HELP (Hierarchical Explainable Latent Pooling), a novel, inherently interpretable graph pooling approach that reveals how concepts from different GNN layers compose to new ones in later steps. HELP is more than 1-WL expressive and is the first non-spectral, end-to-end-learnable, hierarchical graph pooling method that can learn to pool a variable number of arbitrary connected components. We empirically demonstrate that it performs on-par with standard GCNs and popular pooling methods in terms of accuracy while yielding explanations that are aligned with expert knowledge in the domains of chemistry and social networks. In addition to a qualitative analysis, we employ concept completeness scores as well as concept conformity, a novel metric to measure the noise in discovered concepts, quantitatively verifying that the discovered concepts are significantly easier to fully understand than those from previous work. Our work represents a first step towards an understanding of graph neural networks that goes beyond a set of concepts from the final layer and instead explains the complex interplay of concepts on different levels.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Jürß, Jonas and Magister, Lucie Charlotte and Barbiero, Pietro and Liò, Pietro and Simidjievski, Nikola},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lewisOneBiggestProblems2023,
	title = {One of the {Biggest} {Problems} in {Biology} {Has} {Finally} {Been} {Solved}},
	url = {https://www.scientificamerican.com/article/one-of-the-biggest-problems-in-biology-has-finally-been-solved/},
	abstract = {Google DeepMind CEO Demis Hassabis explains how its AlphaFold AI program predicted the 3-D structure of every known protein},
	language = {en},
	urldate = {2023-11-27},
	journal = {Scientific American},
	author = {Lewis, Tanya},
	month = feb,
	year = {2023},
}

@article{jumperAlphaFold2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2023-11-24},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
}

@misc{schutBridgingHumanAIKnowledge2023,
	title = {Bridging the {Human}-{AI} {Knowledge} {Gap}: {Concept} {Discovery} and {Transfer} in {AlphaZero}},
	shorttitle = {Bridging the {Human}-{AI} {Knowledge} {Gap}},
	url = {http://arxiv.org/abs/2310.16410},
	abstract = {Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Schut, Lisa and Tomasev, Nenad and McGrath, Tom and Hassabis, Demis and Paquet, Ulrich and Kim, Been},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16410 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@misc{holtgenDeDUCEGeneratingCounterfactual2021,
	title = {{DeDUCE}: {Generating} {Counterfactual} {Explanations} {Efficiently}},
	shorttitle = {{DeDUCE}},
	url = {http://arxiv.org/abs/2111.15639},
	doi = {10.48550/arXiv.2111.15639},
	abstract = {When an image classifier outputs a wrong class label, it can be helpful to see what changes in the image would lead to a correct classification. This is the aim of algorithms generating counterfactual explanations. However, there is no easily scalable method to generate such counterfactuals. We develop a new algorithm providing counterfactual explanations for large image classifiers trained with spectral normalisation at low computational cost. We empirically compare this algorithm against baselines from the literature; our novel algorithm consistently finds counterfactuals that are much closer to the original inputs. At the same time, the realism of these counterfactuals is comparable to the baselines. The code for all experiments is available at https://github.com/benedikthoeltgen/DeDUCE.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Höltgen, Benedikt and Schut, Lisa and Brauner, Jan M. and Gal, Yarin},
	month = nov,
	year = {2021},
	note = {arXiv:2111.15639 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@inproceedings{kanBrainNetworkTransformer2022,
	title = {Brain {Network} {Transformer}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/a408234a9b80604a9cf6ca518e474550-Abstract-Conference.html},
	booktitle = {{NeurIPS}},
	author = {Kan, Xuan and Dai, Wei and Cui, Hejie and Zhang, Zilong and Guo, Ying and Yang, Carl},
	year = {2022},
}

@inproceedings{nakkiranDeepDoubleDescent2020,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	url = {https://openreview.net/forum?id=B1g5sA4twr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	year = {2020},
}

@misc{powerGrokkingGeneralizationOverfitting2022,
	title = {Grokking: {Generalization} {Beyond} {Overfitting} on {Small} {Algorithmic} {Datasets}},
	shorttitle = {Grokking},
	url = {http://arxiv.org/abs/2201.02177},
	doi = {10.48550/arXiv.2201.02177},
	abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
	urldate = {2023-11-12},
	publisher = {arXiv},
	author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02177 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{footeNeuronGraphInterpreting2023,
	title = {Neuron to {Graph}: {Interpreting} {Language} {Model} {Neurons} at {Scale}},
	shorttitle = {Neuron to {Graph}},
	url = {http://arxiv.org/abs/2305.19911},
	doi = {10.48550/arXiv.2305.19911},
	abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
	month = may,
	year = {2023},
	note = {arXiv:2305.19911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{giovanniOverSquashingMessagePassing2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}: {The} {Impact} of {Width}, {Depth}, and {Topology}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/di-giovanni23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Giovanni, Francesco Di and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio, Pietro and Bronstein, Michael M.},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {7865--7885},
}

@article{finkelshteinSinglenodeAttacksFooling2022,
	title = {Single-node attacks for fooling graph neural networks},
	volume = {513},
	url = {https://doi.org/10.1016/j.neucom.2022.09.115},
	doi = {10.1016/J.NEUCOM.2022.09.115},
	journal = {Neurocomputing},
	author = {Finkelshtein, Ben and Baskin, Chaim and Zheltonozhskii, Evgenii and Alon, Uri},
	year = {2022},
	keywords = {notion},
	pages = {1--12},
}

@misc{lanLocatingCrossTaskSequence2023,
	title = {Locating {Cross}-{Task} {Sequence} {Continuation} {Circuits} in {Transformers}},
	url = {http://arxiv.org/abs/2311.04131},
	doi = {10.48550/arXiv.2311.04131},
	abstract = {While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Lan, Michael and Barez, Fazl},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{gevaTransformerFeedForwardLayers2022,
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {https://doi.org/10.18653/v1/2022.emnlp-main.3},
	doi = {10.18653/V1/2022.EMNLP-MAIN.3},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2022, {Abu} {Dhabi}, {United} {Arab} {Emirates}, {December} 7-11, 2022},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	year = {2022},
	keywords = {notion},
	pages = {30--45},
}

@inproceedings{gevaTransformerFeedForwardLayers2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {https://doi.org/10.18653/v1/2021.emnlp-main.446},
	doi = {10.18653/V1/2021.EMNLP-MAIN.446},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2021, {Virtual} {Event} / {Punta} {Cana}, {Dominican} {Republic}, 7-11 {November}, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	year = {2021},
	keywords = {notion},
	pages = {5484--5495},
}

@inproceedings{chughtaiToyModelUniversality2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Toy} {Model} of {Universality}: {Reverse} {Engineering} how {Networks} {Learn} {Group} {Operations}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/chughtai23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {6243--6267},
}

@inproceedings{krizhevskyAlexNet2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
	year = {2012},
}

@article{olahNaturallyOccurringEquivariance2020,
	title = {Naturally {Occurring} {Equivariance} in {Neural} {Networks}},
	url = {https://distill.pub/2020/circuits/equivariance},
	doi = {10.23915/distill.00024.004},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Voss, Chelsea and Schubert, Ludwig and Goh, Gabriel},
	year = {2020},
	keywords = {notion},
}

@article{cammarataCurveDetectors2020,
	title = {Curve {Detectors}},
	url = {https://distill.pub/2020/circuits/curve-detectors},
	doi = {10.23915/distill.00024.003},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
	year = {2020},
}

@article{olahZoomIntroductionCircuits2020,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	url = {https://distill.pub/2020/circuits/zoom-in},
	doi = {10.23915/distill.00024.001},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
	year = {2020},
	keywords = {notion},
}

@article{petrovWeightBanding2021,
	title = {Weight {Banding}},
	url = {https://distill.pub/2020/circuits/weight-banding},
	doi = {10.23915/distill.00024.009},
	journal = {Distill},
	author = {Petrov, Michael and Voss, Chelsea and Schubert, Ludwig and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{yangHowGNNsLearn2023,
	title = {How {Graph} {Neural} {Networks} {Learn}: {Lessons} from {Training} {Dynamics} in {Function} {Space}},
	shorttitle = {How {Graph} {Neural} {Networks} {Learn}},
	url = {http://arxiv.org/abs/2310.05105},
	abstract = {A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, however it remains less clear whether and how GNNs learn desired functions during the optimization process. To fill this critical gap, we study the learning dynamics of GNNs in function space via the analytic framework of overparameterization. In particular, we find that the seemingly complicated training process of GNNs can be re-cast into a more familiar label propagation framework, due to the graph inductive bias implicit in this process. From this vantage point, we provide explanations for why the learned GNN functions successfully generalize and for their pathological behavior on heterophilic graphs, which are consistent with observations. Practically, sparsifying and implementing the learning dynamics lead to a minimalist semi-supervised learning algorithm with the efficiency of classic algorithms and the effectiveness of modern GNNs.},
	urldate = {2023-10-30},
	publisher = {arXiv},
	author = {Yang, Chenxiao and Wu, Qitian and Wipf, David and Sun, Ruoyu and Yan, Junchi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05105 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
}

@misc{kipfVariationalGraphAutoEncoders2016,
	title = {Variational {Graph} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1611.07308},
	doi = {10.48550/arXiv.1611.07308},
	abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = nov,
	year = {2016},
	note = {arXiv:1611.07308 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@article{olahMechanisticInterpretabilityVariables2022,
	title = {Mechanistic {Interpretability}, {Variables}, and the {Importance} of {Interpretable} {Bases}},
	url = {https://transformer-circuits.pub/2022/mech-interp-essay/},
	journal = {Transformer Circuits Thread},
	author = {Olah, Chris},
	year = {2022},
}

@article{olahFeatureVisualization2017,
	title = {Feature {Visualization}},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	year = {2017},
}

@article{johnsonExtensionsLipschitzMappings1984,
	title = {Extensions of {Lipschitz} mappings into {Hilbert} space},
	volume = {26},
	url = {https://stanford.edu/class/cs114/readings/JL-Johnson.pdf},
	journal = {Contemporary mathematics},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	year = {1984},
	pages = {189--206},
}

@article{aroraLinearAlgebraicStructure2018,
	title = {Linear {Algebraic} {Structure} of {Word} {Senses}, with {Applications} to {Polysemy}},
	volume = {6},
	url = {https://doi.org/10.1162/tacl_a_00034},
	doi = {10.1162/tacl\_a\_00034},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	year = {2018},
	keywords = {notion},
	pages = {483--495},
}

@article{heimersheimCircuitPythonDocstrings2023,
	title = {A circuit for {Python} docstrings in a 4-layer attention-only transformer},
	url = {https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only},
	author = {Heimersheim, Stefan and Janiak, Jett},
	year = {2023},
	keywords = {notion},
}

@article{schubertHighLowFrequencyDetectors2021,
	title = {High-{Low} {Frequency} {Detectors}},
	url = {https://distill.pub/2020/circuits/frequency-edges},
	doi = {10.23915/distill.00024.005},
	journal = {Distill},
	author = {Schubert, Ludwig and Voss, Chelsea and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{cammarataCurveCircuits2021,
	title = {Curve {Circuits}},
	url = {https://distill.pub/2020/circuits/curve-circuits},
	doi = {10.23915/distill.00024.006},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Voss, Chelsea and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{vossBranchSpecialization2021,
	title = {Branch {Specialization}},
	url = {https://distill.pub/2020/circuits/branch-specialization},
	doi = {10.23915/distill.00024.008},
	journal = {Distill},
	author = {Voss, Chelsea and Goh, Gabriel and Cammarata, Nick and Petrov, Michael and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}


@inproceedings{numerosoDualAlgorithmicReasoning2023,
	title = {Dual {Algorithmic} {Reasoning}},
	url = {https://openreview.net/pdf?id=hhvkdRdWt1F},
	urldate = {2024-01-23},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}, {ICLR} 2023, {Kigali}, {Rwanda}, {May} 1-5, 2023},
	publisher = {OpenReview.net},
	author = {Numeroso, Danilo and Bacciu, Davide and Velickovic, Petar},
	year = {2023},
	keywords = {notion},
}

@inproceedings{mengLocatingEditingROME2022,
	title = {Locating and {Editing} {Factual} {Associations} in {GPT}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 35: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2022, {NeurIPS} 2022, {New} {Orleans}, {LA}, {USA}, {November} 28 - {December} 9, 2022},
	author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
	editor = {Koyejo, Sanmi and Mohamed, S. and Agarwal, A. and Belgrave, Danielle and Cho, K. and Oh, A.},
	year = {2022},
	keywords = {notion},
}

@article{huChatGPTSetsRecord2023,
	chapter = {Technology},
	title = {{ChatGPT} sets record for fastest-growing user base - analyst note},
	url = {https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/},
	abstract = {ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after launch, making it the fastest-growing consumer application in history, according to a UBS study on Wednesday.},
	language = {en},
	urldate = {2024-01-14},
	journal = {Reuters},
	author = {Hu, Krystal},
	month = feb,
	year = {2023},
}

@inproceedings{juerssRecursiveAlgorithmicReasoning2023Openreview,
	title = {Recursive {Algorithmic} {Reasoning}},
	url = {https://openreview.net/forum?id=43M1bPorxU},
	booktitle = {The {Second} {Learning} on {Graphs} {Conference}},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja and Veličković, Petar},
	year = {2023},
}

@inproceedings{juerssRecursiveReasoningNeural2023,
	title = {Recursive {Reasoning} with {Neural} {Networks}},
	url = {https://openreview.net/pdf?id=TS8l4VS7_BK},
	booktitle = {The {First} {Tiny} {Papers} {Track} at {ICLR} 2023, {Tiny} {Papers} @ {ICLR} 2023, {Kigali}, {Rwanda}, {May} 5, 2023},
	publisher = {OpenReview.net},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja},
	editor = {Maughan, Krystal and Liu, Rosanne and Burns, Thomas F.},
	year = {2023},
}

@inproceedings{zintgrafVariBADVeryGood2020,
	title = {{VariBAD}: {A} {Very} {Good} {Method} for {Bayes}-{Adaptive} {Deep} {RL} via {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=Hkl9JlBYvr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Zintgraf, Luisa M. and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
	year = {2020},
}

@misc{guMambaLinearTimeSequence2023,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
}

@inproceedings{yasunagaQAGNNReasoningLanguage2021,
	title = {{QA}-{GNN}: {Reasoning} with {Language} {Models} and {Knowledge} {Graphs} for {Question} {Answering}},
	url = {https://doi.org/10.18653/v1/2021.naacl-main.45},
	doi = {10.18653/V1/2021.NAACL-MAIN.45},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2021, {Online}, {June} 6-11, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tür, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	year = {2021},
	keywords = {notion},
	pages = {535--546},
}

@inproceedings{mengGNNLMLanguageModeling2021,
	title = {{GNN}-{LM}: {Language} {Modeling} based on {Global} {Contexts} via {GNN}},
	shorttitle = {{GNN}-{LM}},
	url = {https://openreview.net/forum?id=BS49l-B5Bql},
	abstract = {Inspired by the notion that "it to copy is easier than to memorize", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.},
	language = {en},
	urldate = {2023-12-02},
	author = {Meng, Yuxian and Zong, Shi and Li, Xiaoya and Sun, Xiaofei and Zhang, Tianwei and Wu, Fei and Li, Jiwei},
	month = oct,
	year = {2021},
}

@misc{juerssHELP2023,
	title = {Everybody {Needs} a {Little} {HELP}: {Explaining} {Graphs} via {Hierarchical} {Concepts}},
	shorttitle = {Everybody {Needs} a {Little} {HELP}},
	url = {http://arxiv.org/abs/2311.15112},
	doi = {10.48550/arXiv.2311.15112},
	abstract = {Graph neural networks (GNNs) have led to major breakthroughs in a variety of domains such as drug discovery, social network analysis, and travel time estimation. However, they lack interpretability which hinders human trust and thereby deployment to settings with high-stakes decisions. A line of interpretable methods approach this by discovering a small set of relevant concepts as subgraphs in the last GNN layer that together explain the prediction. This can yield oversimplified explanations, failing to explain the interaction between GNN layers. To address this oversight, we provide HELP (Hierarchical Explainable Latent Pooling), a novel, inherently interpretable graph pooling approach that reveals how concepts from different GNN layers compose to new ones in later steps. HELP is more than 1-WL expressive and is the first non-spectral, end-to-end-learnable, hierarchical graph pooling method that can learn to pool a variable number of arbitrary connected components. We empirically demonstrate that it performs on-par with standard GCNs and popular pooling methods in terms of accuracy while yielding explanations that are aligned with expert knowledge in the domains of chemistry and social networks. In addition to a qualitative analysis, we employ concept completeness scores as well as concept conformity, a novel metric to measure the noise in discovered concepts, quantitatively verifying that the discovered concepts are significantly easier to fully understand than those from previous work. Our work represents a first step towards an understanding of graph neural networks that goes beyond a set of concepts from the final layer and instead explains the complex interplay of concepts on different levels.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Jürß, Jonas and Magister, Lucie Charlotte and Barbiero, Pietro and Liò, Pietro and Simidjievski, Nikola},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lewisOneBiggestProblems2023,
	title = {One of the {Biggest} {Problems} in {Biology} {Has} {Finally} {Been} {Solved}},
	url = {https://www.scientificamerican.com/article/one-of-the-biggest-problems-in-biology-has-finally-been-solved/},
	abstract = {Google DeepMind CEO Demis Hassabis explains how its AlphaFold AI program predicted the 3-D structure of every known protein},
	language = {en},
	urldate = {2023-11-27},
	journal = {Scientific American},
	author = {Lewis, Tanya},
	month = feb,
	year = {2023},
}

@article{jumperAlphaFold2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2023-11-24},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
}

@misc{schutBridgingHumanAIKnowledge2023,
	title = {Bridging the {Human}-{AI} {Knowledge} {Gap}: {Concept} {Discovery} and {Transfer} in {AlphaZero}},
	shorttitle = {Bridging the {Human}-{AI} {Knowledge} {Gap}},
	url = {http://arxiv.org/abs/2310.16410},
	abstract = {Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Schut, Lisa and Tomasev, Nenad and McGrath, Tom and Hassabis, Demis and Paquet, Ulrich and Kim, Been},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16410 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@misc{holtgenDeDUCEGeneratingCounterfactual2021,
	title = {{DeDUCE}: {Generating} {Counterfactual} {Explanations} {Efficiently}},
	shorttitle = {{DeDUCE}},
	url = {http://arxiv.org/abs/2111.15639},
	doi = {10.48550/arXiv.2111.15639},
	abstract = {When an image classifier outputs a wrong class label, it can be helpful to see what changes in the image would lead to a correct classification. This is the aim of algorithms generating counterfactual explanations. However, there is no easily scalable method to generate such counterfactuals. We develop a new algorithm providing counterfactual explanations for large image classifiers trained with spectral normalisation at low computational cost. We empirically compare this algorithm against baselines from the literature; our novel algorithm consistently finds counterfactuals that are much closer to the original inputs. At the same time, the realism of these counterfactuals is comparable to the baselines. The code for all experiments is available at https://github.com/benedikthoeltgen/DeDUCE.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Höltgen, Benedikt and Schut, Lisa and Brauner, Jan M. and Gal, Yarin},
	month = nov,
	year = {2021},
	note = {arXiv:2111.15639 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@inproceedings{kanBrainNetworkTransformer2022,
	title = {Brain {Network} {Transformer}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/a408234a9b80604a9cf6ca518e474550-Abstract-Conference.html},
	booktitle = {{NeurIPS}},
	author = {Kan, Xuan and Dai, Wei and Cui, Hejie and Zhang, Zilong and Guo, Ying and Yang, Carl},
	year = {2022},
}

@inproceedings{nakkiranDeepDoubleDescent2020,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	url = {https://openreview.net/forum?id=B1g5sA4twr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	year = {2020},
}

@misc{powerGrokkingGeneralizationOverfitting2022,
	title = {Grokking: {Generalization} {Beyond} {Overfitting} on {Small} {Algorithmic} {Datasets}},
	shorttitle = {Grokking},
	url = {http://arxiv.org/abs/2201.02177},
	doi = {10.48550/arXiv.2201.02177},
	abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
	urldate = {2023-11-12},
	publisher = {arXiv},
	author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02177 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{footeNeuronGraphInterpreting2023,
	title = {Neuron to {Graph}: {Interpreting} {Language} {Model} {Neurons} at {Scale}},
	shorttitle = {Neuron to {Graph}},
	url = {http://arxiv.org/abs/2305.19911},
	doi = {10.48550/arXiv.2305.19911},
	abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
	month = may,
	year = {2023},
	note = {arXiv:2305.19911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{giovanniOverSquashingMessagePassing2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}: {The} {Impact} of {Width}, {Depth}, and {Topology}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/di-giovanni23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Giovanni, Francesco Di and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio, Pietro and Bronstein, Michael M.},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {7865--7885},
}

@article{finkelshteinSinglenodeAttacksFooling2022,
	title = {Single-node attacks for fooling graph neural networks},
	volume = {513},
	url = {https://doi.org/10.1016/j.neucom.2022.09.115},
	doi = {10.1016/J.NEUCOM.2022.09.115},
	journal = {Neurocomputing},
	author = {Finkelshtein, Ben and Baskin, Chaim and Zheltonozhskii, Evgenii and Alon, Uri},
	year = {2022},
	keywords = {notion},
	pages = {1--12},
}

@misc{lanLocatingCrossTaskSequence2023,
	title = {Locating {Cross}-{Task} {Sequence} {Continuation} {Circuits} in {Transformers}},
	url = {http://arxiv.org/abs/2311.04131},
	doi = {10.48550/arXiv.2311.04131},
	abstract = {While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Lan, Michael and Barez, Fazl},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{gevaTransformerFeedForwardLayers2022,
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {https://doi.org/10.18653/v1/2022.emnlp-main.3},
	doi = {10.18653/V1/2022.EMNLP-MAIN.3},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2022, {Abu} {Dhabi}, {United} {Arab} {Emirates}, {December} 7-11, 2022},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	year = {2022},
	keywords = {notion},
	pages = {30--45},
}

@inproceedings{gevaTransformerFeedForwardLayers2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {https://doi.org/10.18653/v1/2021.emnlp-main.446},
	doi = {10.18653/V1/2021.EMNLP-MAIN.446},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2021, {Virtual} {Event} / {Punta} {Cana}, {Dominican} {Republic}, 7-11 {November}, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	year = {2021},
	keywords = {notion},
	pages = {5484--5495},
}

@inproceedings{chughtaiToyModelUniversality2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Toy} {Model} of {Universality}: {Reverse} {Engineering} how {Networks} {Learn} {Group} {Operations}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/chughtai23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {6243--6267},
}

@inproceedings{krizhevskyAlexNet2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
	year = {2012},
}

@article{olahNaturallyOccurringEquivariance2020,
	title = {Naturally {Occurring} {Equivariance} in {Neural} {Networks}},
	url = {https://distill.pub/2020/circuits/equivariance},
	doi = {10.23915/distill.00024.004},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Voss, Chelsea and Schubert, Ludwig and Goh, Gabriel},
	year = {2020},
	keywords = {notion},
}

@article{cammarataCurveDetectors2020,
	title = {Curve {Detectors}},
	url = {https://distill.pub/2020/circuits/curve-detectors},
	doi = {10.23915/distill.00024.003},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
	year = {2020},
}

@article{olahZoomIntroductionCircuits2020,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	url = {https://distill.pub/2020/circuits/zoom-in},
	doi = {10.23915/distill.00024.001},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
	year = {2020},
	keywords = {notion},
}

@article{petrovWeightBanding2021,
	title = {Weight {Banding}},
	url = {https://distill.pub/2020/circuits/weight-banding},
	doi = {10.23915/distill.00024.009},
	journal = {Distill},
	author = {Petrov, Michael and Voss, Chelsea and Schubert, Ludwig and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{yangHowGNNsLearn2023,
	title = {How {Graph} {Neural} {Networks} {Learn}: {Lessons} from {Training} {Dynamics} in {Function} {Space}},
	shorttitle = {How {Graph} {Neural} {Networks} {Learn}},
	url = {http://arxiv.org/abs/2310.05105},
	abstract = {A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, however it remains less clear whether and how GNNs learn desired functions during the optimization process. To fill this critical gap, we study the learning dynamics of GNNs in function space via the analytic framework of overparameterization. In particular, we find that the seemingly complicated training process of GNNs can be re-cast into a more familiar label propagation framework, due to the graph inductive bias implicit in this process. From this vantage point, we provide explanations for why the learned GNN functions successfully generalize and for their pathological behavior on heterophilic graphs, which are consistent with observations. Practically, sparsifying and implementing the learning dynamics lead to a minimalist semi-supervised learning algorithm with the efficiency of classic algorithms and the effectiveness of modern GNNs.},
	urldate = {2023-10-30},
	publisher = {arXiv},
	author = {Yang, Chenxiao and Wu, Qitian and Wipf, David and Sun, Ruoyu and Yan, Junchi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05105 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
}

@misc{kipfVariationalGraphAutoEncoders2016,
	title = {Variational {Graph} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1611.07308},
	doi = {10.48550/arXiv.1611.07308},
	abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = nov,
	year = {2016},
	note = {arXiv:1611.07308 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@article{olahMechanisticInterpretabilityVariables2022,
	title = {Mechanistic {Interpretability}, {Variables}, and the {Importance} of {Interpretable} {Bases}},
	url = {https://transformer-circuits.pub/2022/mech-interp-essay/},
	journal = {Transformer Circuits Thread},
	author = {Olah, Chris},
	year = {2022},
}

@article{olahFeatureVisualization2017,
	title = {Feature {Visualization}},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	year = {2017},
}

@article{johnsonExtensionsLipschitzMappings1984,
	title = {Extensions of {Lipschitz} mappings into {Hilbert} space},
	volume = {26},
	url = {https://stanford.edu/class/cs114/readings/JL-Johnson.pdf},
	journal = {Contemporary mathematics},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	year = {1984},
	pages = {189--206},
}

@article{aroraLinearAlgebraicStructure2018,
	title = {Linear {Algebraic} {Structure} of {Word} {Senses}, with {Applications} to {Polysemy}},
	volume = {6},
	url = {https://doi.org/10.1162/tacl_a_00034},
	doi = {10.1162/tacl\_a\_00034},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	year = {2018},
	keywords = {notion},
	pages = {483--495},
}

@article{heimersheimCircuitPythonDocstrings2023,
	title = {A circuit for {Python} docstrings in a 4-layer attention-only transformer},
	url = {https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only},
	author = {Heimersheim, Stefan and Janiak, Jett},
	year = {2023},
	keywords = {notion},
}

@article{schubertHighLowFrequencyDetectors2021,
	title = {High-{Low} {Frequency} {Detectors}},
	url = {https://distill.pub/2020/circuits/frequency-edges},
	doi = {10.23915/distill.00024.005},
	journal = {Distill},
	author = {Schubert, Ludwig and Voss, Chelsea and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{cammarataCurveCircuits2021,
	title = {Curve {Circuits}},
	url = {https://distill.pub/2020/circuits/curve-circuits},
	doi = {10.23915/distill.00024.006},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Voss, Chelsea and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{vossBranchSpecialization2021,
	title = {Branch {Specialization}},
	url = {https://distill.pub/2020/circuits/branch-specialization},
	doi = {10.23915/distill.00024.008},
	journal = {Distill},
	author = {Voss, Chelsea and Goh, Gabriel and Cammarata, Nick and Petrov, Michael and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}


@inproceedings{numerosoDualAlgorithmicReasoning2023,
	title = {Dual {Algorithmic} {Reasoning}},
	url = {https://openreview.net/pdf?id=hhvkdRdWt1F},
	urldate = {2024-01-23},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}, {ICLR} 2023, {Kigali}, {Rwanda}, {May} 1-5, 2023},
	publisher = {OpenReview.net},
	author = {Numeroso, Danilo and Bacciu, Davide and Velickovic, Petar},
	year = {2023},
	keywords = {notion},
}

@inproceedings{mengLocatingEditingROME2022,
	title = {Locating and {Editing} {Factual} {Associations} in {GPT}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 35: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2022, {NeurIPS} 2022, {New} {Orleans}, {LA}, {USA}, {November} 28 - {December} 9, 2022},
	author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
	editor = {Koyejo, Sanmi and Mohamed, S. and Agarwal, A. and Belgrave, Danielle and Cho, K. and Oh, A.},
	year = {2022},
	keywords = {notion},
}

@article{huChatGPTSetsRecord2023,
	chapter = {Technology},
	title = {{ChatGPT} sets record for fastest-growing user base - analyst note},
	url = {https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/},
	abstract = {ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after launch, making it the fastest-growing consumer application in history, according to a UBS study on Wednesday.},
	language = {en},
	urldate = {2024-01-14},
	journal = {Reuters},
	author = {Hu, Krystal},
	month = feb,
	year = {2023},
}

@inproceedings{juerssRecursiveAlgorithmicReasoning2023Openreview,
	title = {Recursive {Algorithmic} {Reasoning}},
	url = {https://openreview.net/forum?id=43M1bPorxU},
	booktitle = {The {Second} {Learning} on {Graphs} {Conference}},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja and Veličković, Petar},
	year = {2023},
}

@inproceedings{juerssRecursiveReasoningNeural2023,
	title = {Recursive {Reasoning} with {Neural} {Networks}},
	url = {https://openreview.net/pdf?id=TS8l4VS7_BK},
	booktitle = {The {First} {Tiny} {Papers} {Track} at {ICLR} 2023, {Tiny} {Papers} @ {ICLR} 2023, {Kigali}, {Rwanda}, {May} 5, 2023},
	publisher = {OpenReview.net},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja},
	editor = {Maughan, Krystal and Liu, Rosanne and Burns, Thomas F.},
	year = {2023},
}

@inproceedings{zintgrafVariBADVeryGood2020,
	title = {{VariBAD}: {A} {Very} {Good} {Method} for {Bayes}-{Adaptive} {Deep} {RL} via {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=Hkl9JlBYvr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Zintgraf, Luisa M. and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
	year = {2020},
}

@misc{guMambaLinearTimeSequence2023,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
}

@inproceedings{yasunagaQAGNNReasoningLanguage2021,
	title = {{QA}-{GNN}: {Reasoning} with {Language} {Models} and {Knowledge} {Graphs} for {Question} {Answering}},
	url = {https://doi.org/10.18653/v1/2021.naacl-main.45},
	doi = {10.18653/V1/2021.NAACL-MAIN.45},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2021, {Online}, {June} 6-11, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tür, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	year = {2021},
	keywords = {notion},
	pages = {535--546},
}

@inproceedings{mengGNNLMLanguageModeling2021,
	title = {{GNN}-{LM}: {Language} {Modeling} based on {Global} {Contexts} via {GNN}},
	shorttitle = {{GNN}-{LM}},
	url = {https://openreview.net/forum?id=BS49l-B5Bql},
	abstract = {Inspired by the notion that "it to copy is easier than to memorize", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.},
	language = {en},
	urldate = {2023-12-02},
	author = {Meng, Yuxian and Zong, Shi and Li, Xiaoya and Sun, Xiaofei and Zhang, Tianwei and Wu, Fei and Li, Jiwei},
	month = oct,
	year = {2021},
}

@misc{juerssHELP2023,
	title = {Everybody {Needs} a {Little} {HELP}: {Explaining} {Graphs} via {Hierarchical} {Concepts}},
	shorttitle = {Everybody {Needs} a {Little} {HELP}},
	url = {http://arxiv.org/abs/2311.15112},
	doi = {10.48550/arXiv.2311.15112},
	abstract = {Graph neural networks (GNNs) have led to major breakthroughs in a variety of domains such as drug discovery, social network analysis, and travel time estimation. However, they lack interpretability which hinders human trust and thereby deployment to settings with high-stakes decisions. A line of interpretable methods approach this by discovering a small set of relevant concepts as subgraphs in the last GNN layer that together explain the prediction. This can yield oversimplified explanations, failing to explain the interaction between GNN layers. To address this oversight, we provide HELP (Hierarchical Explainable Latent Pooling), a novel, inherently interpretable graph pooling approach that reveals how concepts from different GNN layers compose to new ones in later steps. HELP is more than 1-WL expressive and is the first non-spectral, end-to-end-learnable, hierarchical graph pooling method that can learn to pool a variable number of arbitrary connected components. We empirically demonstrate that it performs on-par with standard GCNs and popular pooling methods in terms of accuracy while yielding explanations that are aligned with expert knowledge in the domains of chemistry and social networks. In addition to a qualitative analysis, we employ concept completeness scores as well as concept conformity, a novel metric to measure the noise in discovered concepts, quantitatively verifying that the discovered concepts are significantly easier to fully understand than those from previous work. Our work represents a first step towards an understanding of graph neural networks that goes beyond a set of concepts from the final layer and instead explains the complex interplay of concepts on different levels.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Jürß, Jonas and Magister, Lucie Charlotte and Barbiero, Pietro and Liò, Pietro and Simidjievski, Nikola},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lewisOneBiggestProblems2023,
	title = {One of the {Biggest} {Problems} in {Biology} {Has} {Finally} {Been} {Solved}},
	url = {https://www.scientificamerican.com/article/one-of-the-biggest-problems-in-biology-has-finally-been-solved/},
	abstract = {Google DeepMind CEO Demis Hassabis explains how its AlphaFold AI program predicted the 3-D structure of every known protein},
	language = {en},
	urldate = {2023-11-27},
	journal = {Scientific American},
	author = {Lewis, Tanya},
	month = feb,
	year = {2023},
}

@article{jumperAlphaFold2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2023-11-24},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
}

@misc{schutBridgingHumanAIKnowledge2023,
	title = {Bridging the {Human}-{AI} {Knowledge} {Gap}: {Concept} {Discovery} and {Transfer} in {AlphaZero}},
	shorttitle = {Bridging the {Human}-{AI} {Knowledge} {Gap}},
	url = {http://arxiv.org/abs/2310.16410},
	abstract = {Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Schut, Lisa and Tomasev, Nenad and McGrath, Tom and Hassabis, Demis and Paquet, Ulrich and Kim, Been},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16410 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@misc{holtgenDeDUCEGeneratingCounterfactual2021,
	title = {{DeDUCE}: {Generating} {Counterfactual} {Explanations} {Efficiently}},
	shorttitle = {{DeDUCE}},
	url = {http://arxiv.org/abs/2111.15639},
	doi = {10.48550/arXiv.2111.15639},
	abstract = {When an image classifier outputs a wrong class label, it can be helpful to see what changes in the image would lead to a correct classification. This is the aim of algorithms generating counterfactual explanations. However, there is no easily scalable method to generate such counterfactuals. We develop a new algorithm providing counterfactual explanations for large image classifiers trained with spectral normalisation at low computational cost. We empirically compare this algorithm against baselines from the literature; our novel algorithm consistently finds counterfactuals that are much closer to the original inputs. At the same time, the realism of these counterfactuals is comparable to the baselines. The code for all experiments is available at https://github.com/benedikthoeltgen/DeDUCE.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Höltgen, Benedikt and Schut, Lisa and Brauner, Jan M. and Gal, Yarin},
	month = nov,
	year = {2021},
	note = {arXiv:2111.15639 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@inproceedings{kanBrainNetworkTransformer2022,
	title = {Brain {Network} {Transformer}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/a408234a9b80604a9cf6ca518e474550-Abstract-Conference.html},
	booktitle = {{NeurIPS}},
	author = {Kan, Xuan and Dai, Wei and Cui, Hejie and Zhang, Zilong and Guo, Ying and Yang, Carl},
	year = {2022},
}

@inproceedings{nakkiranDeepDoubleDescent2020,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	url = {https://openreview.net/forum?id=B1g5sA4twr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	year = {2020},
}

@misc{powerGrokkingGeneralizationOverfitting2022,
	title = {Grokking: {Generalization} {Beyond} {Overfitting} on {Small} {Algorithmic} {Datasets}},
	shorttitle = {Grokking},
	url = {http://arxiv.org/abs/2201.02177},
	doi = {10.48550/arXiv.2201.02177},
	abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
	urldate = {2023-11-12},
	publisher = {arXiv},
	author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02177 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{footeNeuronGraphInterpreting2023,
	title = {Neuron to {Graph}: {Interpreting} {Language} {Model} {Neurons} at {Scale}},
	shorttitle = {Neuron to {Graph}},
	url = {http://arxiv.org/abs/2305.19911},
	doi = {10.48550/arXiv.2305.19911},
	abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
	month = may,
	year = {2023},
	note = {arXiv:2305.19911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{giovanniOverSquashingMessagePassing2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}: {The} {Impact} of {Width}, {Depth}, and {Topology}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/di-giovanni23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Giovanni, Francesco Di and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio, Pietro and Bronstein, Michael M.},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {7865--7885},
}

@article{finkelshteinSinglenodeAttacksFooling2022,
	title = {Single-node attacks for fooling graph neural networks},
	volume = {513},
	url = {https://doi.org/10.1016/j.neucom.2022.09.115},
	doi = {10.1016/J.NEUCOM.2022.09.115},
	journal = {Neurocomputing},
	author = {Finkelshtein, Ben and Baskin, Chaim and Zheltonozhskii, Evgenii and Alon, Uri},
	year = {2022},
	keywords = {notion},
	pages = {1--12},
}

@misc{lanLocatingCrossTaskSequence2023,
	title = {Locating {Cross}-{Task} {Sequence} {Continuation} {Circuits} in {Transformers}},
	url = {http://arxiv.org/abs/2311.04131},
	doi = {10.48550/arXiv.2311.04131},
	abstract = {While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Lan, Michael and Barez, Fazl},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{gevaTransformerFeedForwardLayers2022,
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {https://doi.org/10.18653/v1/2022.emnlp-main.3},
	doi = {10.18653/V1/2022.EMNLP-MAIN.3},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2022, {Abu} {Dhabi}, {United} {Arab} {Emirates}, {December} 7-11, 2022},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	year = {2022},
	keywords = {notion},
	pages = {30--45},
}

@inproceedings{gevaTransformerFeedForwardLayers2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {https://doi.org/10.18653/v1/2021.emnlp-main.446},
	doi = {10.18653/V1/2021.EMNLP-MAIN.446},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2021, {Virtual} {Event} / {Punta} {Cana}, {Dominican} {Republic}, 7-11 {November}, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	year = {2021},
	keywords = {notion},
	pages = {5484--5495},
}

@inproceedings{chughtaiToyModelUniversality2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Toy} {Model} of {Universality}: {Reverse} {Engineering} how {Networks} {Learn} {Group} {Operations}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/chughtai23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {6243--6267},
}

@inproceedings{krizhevskyAlexNet2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
	year = {2012},
}

@article{olahNaturallyOccurringEquivariance2020,
	title = {Naturally {Occurring} {Equivariance} in {Neural} {Networks}},
	url = {https://distill.pub/2020/circuits/equivariance},
	doi = {10.23915/distill.00024.004},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Voss, Chelsea and Schubert, Ludwig and Goh, Gabriel},
	year = {2020},
	keywords = {notion},
}

@article{cammarataCurveDetectors2020,
	title = {Curve {Detectors}},
	url = {https://distill.pub/2020/circuits/curve-detectors},
	doi = {10.23915/distill.00024.003},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
	year = {2020},
}

@article{olahZoomIntroductionCircuits2020,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	url = {https://distill.pub/2020/circuits/zoom-in},
	doi = {10.23915/distill.00024.001},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
	year = {2020},
	keywords = {notion},
}

@article{petrovWeightBanding2021,
	title = {Weight {Banding}},
	url = {https://distill.pub/2020/circuits/weight-banding},
	doi = {10.23915/distill.00024.009},
	journal = {Distill},
	author = {Petrov, Michael and Voss, Chelsea and Schubert, Ludwig and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{yangHowGNNsLearn2023,
	title = {How {Graph} {Neural} {Networks} {Learn}: {Lessons} from {Training} {Dynamics} in {Function} {Space}},
	shorttitle = {How {Graph} {Neural} {Networks} {Learn}},
	url = {http://arxiv.org/abs/2310.05105},
	abstract = {A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, however it remains less clear whether and how GNNs learn desired functions during the optimization process. To fill this critical gap, we study the learning dynamics of GNNs in function space via the analytic framework of overparameterization. In particular, we find that the seemingly complicated training process of GNNs can be re-cast into a more familiar label propagation framework, due to the graph inductive bias implicit in this process. From this vantage point, we provide explanations for why the learned GNN functions successfully generalize and for their pathological behavior on heterophilic graphs, which are consistent with observations. Practically, sparsifying and implementing the learning dynamics lead to a minimalist semi-supervised learning algorithm with the efficiency of classic algorithms and the effectiveness of modern GNNs.},
	urldate = {2023-10-30},
	publisher = {arXiv},
	author = {Yang, Chenxiao and Wu, Qitian and Wipf, David and Sun, Ruoyu and Yan, Junchi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05105 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
}

@misc{kipfVariationalGraphAutoEncoders2016,
	title = {Variational {Graph} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1611.07308},
	doi = {10.48550/arXiv.1611.07308},
	abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = nov,
	year = {2016},
	note = {arXiv:1611.07308 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@article{olahMechanisticInterpretabilityVariables2022,
	title = {Mechanistic {Interpretability}, {Variables}, and the {Importance} of {Interpretable} {Bases}},
	url = {https://transformer-circuits.pub/2022/mech-interp-essay/},
	journal = {Transformer Circuits Thread},
	author = {Olah, Chris},
	year = {2022},
}

@article{olahFeatureVisualization2017,
	title = {Feature {Visualization}},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	year = {2017},
}

@article{johnsonExtensionsLipschitzMappings1984,
	title = {Extensions of {Lipschitz} mappings into {Hilbert} space},
	volume = {26},
	url = {https://stanford.edu/class/cs114/readings/JL-Johnson.pdf},
	journal = {Contemporary mathematics},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	year = {1984},
	pages = {189--206},
}

@article{aroraLinearAlgebraicStructure2018,
	title = {Linear {Algebraic} {Structure} of {Word} {Senses}, with {Applications} to {Polysemy}},
	volume = {6},
	url = {https://doi.org/10.1162/tacl_a_00034},
	doi = {10.1162/tacl\_a\_00034},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	year = {2018},
	keywords = {notion},
	pages = {483--495},
}

@article{heimersheimCircuitPythonDocstrings2023,
	title = {A circuit for {Python} docstrings in a 4-layer attention-only transformer},
	url = {https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only},
	author = {Heimersheim, Stefan and Janiak, Jett},
	year = {2023},
	keywords = {notion},
}

@article{schubertHighLowFrequencyDetectors2021,
	title = {High-{Low} {Frequency} {Detectors}},
	url = {https://distill.pub/2020/circuits/frequency-edges},
	doi = {10.23915/distill.00024.005},
	journal = {Distill},
	author = {Schubert, Ludwig and Voss, Chelsea and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{cammarataCurveCircuits2021,
	title = {Curve {Circuits}},
	url = {https://distill.pub/2020/circuits/curve-circuits},
	doi = {10.23915/distill.00024.006},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Voss, Chelsea and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{vossBranchSpecialization2021,
	title = {Branch {Specialization}},
	url = {https://distill.pub/2020/circuits/branch-specialization},
	doi = {10.23915/distill.00024.008},
	journal = {Distill},
	author = {Voss, Chelsea and Goh, Gabriel and Cammarata, Nick and Petrov, Michael and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{lindnerTracrCompiledTransformers2023,
	title = {Tracr: {Compiled} {Transformers} as a {Laboratory} for {Interpretability}},
	shorttitle = {Tracr},
	url = {http://arxiv.org/abs/2301.05062},
	abstract = {We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Lindner, David and Kramár, János and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Thomas and Mikulik, Vladimir},
	month = jun,
	year = {2023},
	note = {arXiv:2301.05062 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}


@inproceedings{mengLocatingEditingROME2022,
	title = {Locating and {Editing} {Factual} {Associations} in {GPT}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 35: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2022, {NeurIPS} 2022, {New} {Orleans}, {LA}, {USA}, {November} 28 - {December} 9, 2022},
	author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
	editor = {Koyejo, Sanmi and Mohamed, S. and Agarwal, A. and Belgrave, Danielle and Cho, K. and Oh, A.},
	year = {2022},
	keywords = {notion},
}

@article{huChatGPTSetsRecord2023,
	chapter = {Technology},
	title = {{ChatGPT} sets record for fastest-growing user base - analyst note},
	url = {https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/},
	abstract = {ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after launch, making it the fastest-growing consumer application in history, according to a UBS study on Wednesday.},
	language = {en},
	urldate = {2024-01-14},
	journal = {Reuters},
	author = {Hu, Krystal},
	month = feb,
	year = {2023},
}

@inproceedings{juerssRecursiveAlgorithmicReasoning2023Openreview,
	title = {Recursive {Algorithmic} {Reasoning}},
	url = {https://openreview.net/forum?id=43M1bPorxU},
	booktitle = {The {Second} {Learning} on {Graphs} {Conference}},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja and Veličković, Petar},
	year = {2023},
}

@inproceedings{juerssRecursiveReasoningNeural2023,
	title = {Recursive {Reasoning} with {Neural} {Networks}},
	url = {https://openreview.net/pdf?id=TS8l4VS7_BK},
	booktitle = {The {First} {Tiny} {Papers} {Track} at {ICLR} 2023, {Tiny} {Papers} @ {ICLR} 2023, {Kigali}, {Rwanda}, {May} 5, 2023},
	publisher = {OpenReview.net},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja},
	editor = {Maughan, Krystal and Liu, Rosanne and Burns, Thomas F.},
	year = {2023},
}

@inproceedings{zintgrafVariBADVeryGood2020,
	title = {{VariBAD}: {A} {Very} {Good} {Method} for {Bayes}-{Adaptive} {Deep} {RL} via {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=Hkl9JlBYvr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Zintgraf, Luisa M. and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
	year = {2020},
}

@misc{guMambaLinearTimeSequence2023,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
}

@inproceedings{yasunagaQAGNNReasoningLanguage2021,
	title = {{QA}-{GNN}: {Reasoning} with {Language} {Models} and {Knowledge} {Graphs} for {Question} {Answering}},
	url = {https://doi.org/10.18653/v1/2021.naacl-main.45},
	doi = {10.18653/V1/2021.NAACL-MAIN.45},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2021, {Online}, {June} 6-11, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tür, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	year = {2021},
	keywords = {notion},
	pages = {535--546},
}

@inproceedings{mengGNNLMLanguageModeling2021,
	title = {{GNN}-{LM}: {Language} {Modeling} based on {Global} {Contexts} via {GNN}},
	shorttitle = {{GNN}-{LM}},
	url = {https://openreview.net/forum?id=BS49l-B5Bql},
	abstract = {Inspired by the notion that "it to copy is easier than to memorize", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.},
	language = {en},
	urldate = {2023-12-02},
	author = {Meng, Yuxian and Zong, Shi and Li, Xiaoya and Sun, Xiaofei and Zhang, Tianwei and Wu, Fei and Li, Jiwei},
	month = oct,
	year = {2021},
}

@misc{juerssHELP2023,
	title = {Everybody {Needs} a {Little} {HELP}: {Explaining} {Graphs} via {Hierarchical} {Concepts}},
	shorttitle = {Everybody {Needs} a {Little} {HELP}},
	url = {http://arxiv.org/abs/2311.15112},
	doi = {10.48550/arXiv.2311.15112},
	abstract = {Graph neural networks (GNNs) have led to major breakthroughs in a variety of domains such as drug discovery, social network analysis, and travel time estimation. However, they lack interpretability which hinders human trust and thereby deployment to settings with high-stakes decisions. A line of interpretable methods approach this by discovering a small set of relevant concepts as subgraphs in the last GNN layer that together explain the prediction. This can yield oversimplified explanations, failing to explain the interaction between GNN layers. To address this oversight, we provide HELP (Hierarchical Explainable Latent Pooling), a novel, inherently interpretable graph pooling approach that reveals how concepts from different GNN layers compose to new ones in later steps. HELP is more than 1-WL expressive and is the first non-spectral, end-to-end-learnable, hierarchical graph pooling method that can learn to pool a variable number of arbitrary connected components. We empirically demonstrate that it performs on-par with standard GCNs and popular pooling methods in terms of accuracy while yielding explanations that are aligned with expert knowledge in the domains of chemistry and social networks. In addition to a qualitative analysis, we employ concept completeness scores as well as concept conformity, a novel metric to measure the noise in discovered concepts, quantitatively verifying that the discovered concepts are significantly easier to fully understand than those from previous work. Our work represents a first step towards an understanding of graph neural networks that goes beyond a set of concepts from the final layer and instead explains the complex interplay of concepts on different levels.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Jürß, Jonas and Magister, Lucie Charlotte and Barbiero, Pietro and Liò, Pietro and Simidjievski, Nikola},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lewisOneBiggestProblems2023,
	title = {One of the {Biggest} {Problems} in {Biology} {Has} {Finally} {Been} {Solved}},
	url = {https://www.scientificamerican.com/article/one-of-the-biggest-problems-in-biology-has-finally-been-solved/},
	abstract = {Google DeepMind CEO Demis Hassabis explains how its AlphaFold AI program predicted the 3-D structure of every known protein},
	language = {en},
	urldate = {2023-11-27},
	journal = {Scientific American},
	author = {Lewis, Tanya},
	month = feb,
	year = {2023},
}

@article{jumperAlphaFold2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2023-11-24},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
}

@misc{schutBridgingHumanAIKnowledge2023,
	title = {Bridging the {Human}-{AI} {Knowledge} {Gap}: {Concept} {Discovery} and {Transfer} in {AlphaZero}},
	shorttitle = {Bridging the {Human}-{AI} {Knowledge} {Gap}},
	url = {http://arxiv.org/abs/2310.16410},
	abstract = {Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Schut, Lisa and Tomasev, Nenad and McGrath, Tom and Hassabis, Demis and Paquet, Ulrich and Kim, Been},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16410 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@misc{holtgenDeDUCEGeneratingCounterfactual2021,
	title = {{DeDUCE}: {Generating} {Counterfactual} {Explanations} {Efficiently}},
	shorttitle = {{DeDUCE}},
	url = {http://arxiv.org/abs/2111.15639},
	doi = {10.48550/arXiv.2111.15639},
	abstract = {When an image classifier outputs a wrong class label, it can be helpful to see what changes in the image would lead to a correct classification. This is the aim of algorithms generating counterfactual explanations. However, there is no easily scalable method to generate such counterfactuals. We develop a new algorithm providing counterfactual explanations for large image classifiers trained with spectral normalisation at low computational cost. We empirically compare this algorithm against baselines from the literature; our novel algorithm consistently finds counterfactuals that are much closer to the original inputs. At the same time, the realism of these counterfactuals is comparable to the baselines. The code for all experiments is available at https://github.com/benedikthoeltgen/DeDUCE.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Höltgen, Benedikt and Schut, Lisa and Brauner, Jan M. and Gal, Yarin},
	month = nov,
	year = {2021},
	note = {arXiv:2111.15639 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@inproceedings{kanBrainNetworkTransformer2022,
	title = {Brain {Network} {Transformer}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/a408234a9b80604a9cf6ca518e474550-Abstract-Conference.html},
	booktitle = {{NeurIPS}},
	author = {Kan, Xuan and Dai, Wei and Cui, Hejie and Zhang, Zilong and Guo, Ying and Yang, Carl},
	year = {2022},
}

@inproceedings{nakkiranDeepDoubleDescent2020,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	url = {https://openreview.net/forum?id=B1g5sA4twr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	year = {2020},
}

@misc{powerGrokkingGeneralizationOverfitting2022,
	title = {Grokking: {Generalization} {Beyond} {Overfitting} on {Small} {Algorithmic} {Datasets}},
	shorttitle = {Grokking},
	url = {http://arxiv.org/abs/2201.02177},
	doi = {10.48550/arXiv.2201.02177},
	abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
	urldate = {2023-11-12},
	publisher = {arXiv},
	author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02177 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{footeNeuronGraphInterpreting2023,
	title = {Neuron to {Graph}: {Interpreting} {Language} {Model} {Neurons} at {Scale}},
	shorttitle = {Neuron to {Graph}},
	url = {http://arxiv.org/abs/2305.19911},
	doi = {10.48550/arXiv.2305.19911},
	abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
	month = may,
	year = {2023},
	note = {arXiv:2305.19911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{giovanniOverSquashingMessagePassing2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}: {The} {Impact} of {Width}, {Depth}, and {Topology}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/di-giovanni23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Giovanni, Francesco Di and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio, Pietro and Bronstein, Michael M.},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {7865--7885},
}

@article{finkelshteinSinglenodeAttacksFooling2022,
	title = {Single-node attacks for fooling graph neural networks},
	volume = {513},
	url = {https://doi.org/10.1016/j.neucom.2022.09.115},
	doi = {10.1016/J.NEUCOM.2022.09.115},
	journal = {Neurocomputing},
	author = {Finkelshtein, Ben and Baskin, Chaim and Zheltonozhskii, Evgenii and Alon, Uri},
	year = {2022},
	keywords = {notion},
	pages = {1--12},
}

@misc{lanLocatingCrossTaskSequence2023,
	title = {Locating {Cross}-{Task} {Sequence} {Continuation} {Circuits} in {Transformers}},
	url = {http://arxiv.org/abs/2311.04131},
	doi = {10.48550/arXiv.2311.04131},
	abstract = {While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Lan, Michael and Barez, Fazl},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{gevaTransformerFeedForwardLayers2022,
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {https://doi.org/10.18653/v1/2022.emnlp-main.3},
	doi = {10.18653/V1/2022.EMNLP-MAIN.3},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2022, {Abu} {Dhabi}, {United} {Arab} {Emirates}, {December} 7-11, 2022},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	year = {2022},
	keywords = {notion},
	pages = {30--45},
}

@inproceedings{gevaTransformerFeedForwardLayers2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {https://doi.org/10.18653/v1/2021.emnlp-main.446},
	doi = {10.18653/V1/2021.EMNLP-MAIN.446},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2021, {Virtual} {Event} / {Punta} {Cana}, {Dominican} {Republic}, 7-11 {November}, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	year = {2021},
	keywords = {notion},
	pages = {5484--5495},
}

@inproceedings{chughtaiToyModelUniversality2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Toy} {Model} of {Universality}: {Reverse} {Engineering} how {Networks} {Learn} {Group} {Operations}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/chughtai23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {6243--6267},
}

@inproceedings{krizhevskyAlexNet2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
	year = {2012},
}

@article{olahNaturallyOccurringEquivariance2020,
	title = {Naturally {Occurring} {Equivariance} in {Neural} {Networks}},
	url = {https://distill.pub/2020/circuits/equivariance},
	doi = {10.23915/distill.00024.004},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Voss, Chelsea and Schubert, Ludwig and Goh, Gabriel},
	year = {2020},
	keywords = {notion},
}

@article{cammarataCurveDetectors2020,
	title = {Curve {Detectors}},
	url = {https://distill.pub/2020/circuits/curve-detectors},
	doi = {10.23915/distill.00024.003},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
	year = {2020},
}

@article{olahZoomIntroductionCircuits2020,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	url = {https://distill.pub/2020/circuits/zoom-in},
	doi = {10.23915/distill.00024.001},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
	year = {2020},
	keywords = {notion},
}

@article{petrovWeightBanding2021,
	title = {Weight {Banding}},
	url = {https://distill.pub/2020/circuits/weight-banding},
	doi = {10.23915/distill.00024.009},
	journal = {Distill},
	author = {Petrov, Michael and Voss, Chelsea and Schubert, Ludwig and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{yangHowGNNsLearn2023,
	title = {How {Graph} {Neural} {Networks} {Learn}: {Lessons} from {Training} {Dynamics} in {Function} {Space}},
	shorttitle = {How {Graph} {Neural} {Networks} {Learn}},
	url = {http://arxiv.org/abs/2310.05105},
	abstract = {A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, however it remains less clear whether and how GNNs learn desired functions during the optimization process. To fill this critical gap, we study the learning dynamics of GNNs in function space via the analytic framework of overparameterization. In particular, we find that the seemingly complicated training process of GNNs can be re-cast into a more familiar label propagation framework, due to the graph inductive bias implicit in this process. From this vantage point, we provide explanations for why the learned GNN functions successfully generalize and for their pathological behavior on heterophilic graphs, which are consistent with observations. Practically, sparsifying and implementing the learning dynamics lead to a minimalist semi-supervised learning algorithm with the efficiency of classic algorithms and the effectiveness of modern GNNs.},
	urldate = {2023-10-30},
	publisher = {arXiv},
	author = {Yang, Chenxiao and Wu, Qitian and Wipf, David and Sun, Ruoyu and Yan, Junchi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05105 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
}

@misc{kipfVariationalGraphAutoEncoders2016,
	title = {Variational {Graph} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1611.07308},
	doi = {10.48550/arXiv.1611.07308},
	abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = nov,
	year = {2016},
	note = {arXiv:1611.07308 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@article{olahMechanisticInterpretabilityVariables2022,
	title = {Mechanistic {Interpretability}, {Variables}, and the {Importance} of {Interpretable} {Bases}},
	url = {https://transformer-circuits.pub/2022/mech-interp-essay/},
	journal = {Transformer Circuits Thread},
	author = {Olah, Chris},
	year = {2022},
}

@article{olahFeatureVisualization2017,
	title = {Feature {Visualization}},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	year = {2017},
}

@article{johnsonExtensionsLipschitzMappings1984,
	title = {Extensions of {Lipschitz} mappings into {Hilbert} space},
	volume = {26},
	url = {https://stanford.edu/class/cs114/readings/JL-Johnson.pdf},
	journal = {Contemporary mathematics},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	year = {1984},
	pages = {189--206},
}

@article{aroraLinearAlgebraicStructure2018,
	title = {Linear {Algebraic} {Structure} of {Word} {Senses}, with {Applications} to {Polysemy}},
	volume = {6},
	url = {https://doi.org/10.1162/tacl_a_00034},
	doi = {10.1162/tacl\_a\_00034},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	year = {2018},
	keywords = {notion},
	pages = {483--495},
}

@article{heimersheimCircuitPythonDocstrings2023,
	title = {A circuit for {Python} docstrings in a 4-layer attention-only transformer},
	url = {https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only},
	author = {Heimersheim, Stefan and Janiak, Jett},
	year = {2023},
	keywords = {notion},
}

@article{schubertHighLowFrequencyDetectors2021,
	title = {High-{Low} {Frequency} {Detectors}},
	url = {https://distill.pub/2020/circuits/frequency-edges},
	doi = {10.23915/distill.00024.005},
	journal = {Distill},
	author = {Schubert, Ludwig and Voss, Chelsea and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{cammarataCurveCircuits2021,
	title = {Curve {Circuits}},
	url = {https://distill.pub/2020/circuits/curve-circuits},
	doi = {10.23915/distill.00024.006},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Voss, Chelsea and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{vossBranchSpecialization2021,
	title = {Branch {Specialization}},
	url = {https://distill.pub/2020/circuits/branch-specialization},
	doi = {10.23915/distill.00024.008},
	journal = {Distill},
	author = {Voss, Chelsea and Goh, Gabriel and Cammarata, Nick and Petrov, Michael and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{lindnerTracrCompiledTransformers2023,
	title = {Tracr: {Compiled} {Transformers} as a {Laboratory} for {Interpretability}},
	shorttitle = {Tracr},
	url = {http://arxiv.org/abs/2301.05062},
	abstract = {We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Lindner, David and Kramár, János and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Thomas and Mikulik, Vladimir},
	month = jun,
	year = {2023},
	note = {arXiv:2301.05062 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}


@inproceedings{mengLocatingEditingROME2022,
	title = {Locating and {Editing} {Factual} {Associations} in {GPT}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 35: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2022, {NeurIPS} 2022, {New} {Orleans}, {LA}, {USA}, {November} 28 - {December} 9, 2022},
	author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
	editor = {Koyejo, Sanmi and Mohamed, S. and Agarwal, A. and Belgrave, Danielle and Cho, K. and Oh, A.},
	year = {2022},
	keywords = {notion},
}

@article{huChatGPTSetsRecord2023,
	chapter = {Technology},
	title = {{ChatGPT} sets record for fastest-growing user base - analyst note},
	url = {https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/},
	abstract = {ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after launch, making it the fastest-growing consumer application in history, according to a UBS study on Wednesday.},
	language = {en},
	urldate = {2024-01-14},
	journal = {Reuters},
	author = {Hu, Krystal},
	month = feb,
	year = {2023},
}

@inproceedings{juerssRecursiveAlgorithmicReasoning2023Openreview,
	title = {Recursive {Algorithmic} {Reasoning}},
	url = {https://openreview.net/forum?id=43M1bPorxU},
	booktitle = {The {Second} {Learning} on {Graphs} {Conference}},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja and Veličković, Petar},
	year = {2023},
}

@inproceedings{juerssRecursiveReasoningNeural2023,
	title = {Recursive {Reasoning} with {Neural} {Networks}},
	url = {https://openreview.net/pdf?id=TS8l4VS7_BK},
	booktitle = {The {First} {Tiny} {Papers} {Track} at {ICLR} 2023, {Tiny} {Papers} @ {ICLR} 2023, {Kigali}, {Rwanda}, {May} 5, 2023},
	publisher = {OpenReview.net},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja},
	editor = {Maughan, Krystal and Liu, Rosanne and Burns, Thomas F.},
	year = {2023},
}

@inproceedings{zintgrafVariBADVeryGood2020,
	title = {{VariBAD}: {A} {Very} {Good} {Method} for {Bayes}-{Adaptive} {Deep} {RL} via {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=Hkl9JlBYvr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Zintgraf, Luisa M. and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
	year = {2020},
}

@misc{guMambaLinearTimeSequence2023,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
}

@inproceedings{yasunagaQAGNNReasoningLanguage2021,
	title = {{QA}-{GNN}: {Reasoning} with {Language} {Models} and {Knowledge} {Graphs} for {Question} {Answering}},
	url = {https://doi.org/10.18653/v1/2021.naacl-main.45},
	doi = {10.18653/V1/2021.NAACL-MAIN.45},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2021, {Online}, {June} 6-11, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tür, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	year = {2021},
	keywords = {notion},
	pages = {535--546},
}

@inproceedings{mengGNNLMLanguageModeling2021,
	title = {{GNN}-{LM}: {Language} {Modeling} based on {Global} {Contexts} via {GNN}},
	shorttitle = {{GNN}-{LM}},
	url = {https://openreview.net/forum?id=BS49l-B5Bql},
	abstract = {Inspired by the notion that "it to copy is easier than to memorize", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.},
	language = {en},
	urldate = {2023-12-02},
	author = {Meng, Yuxian and Zong, Shi and Li, Xiaoya and Sun, Xiaofei and Zhang, Tianwei and Wu, Fei and Li, Jiwei},
	month = oct,
	year = {2021},
}

@misc{juerssHELP2023,
	title = {Everybody {Needs} a {Little} {HELP}: {Explaining} {Graphs} via {Hierarchical} {Concepts}},
	shorttitle = {Everybody {Needs} a {Little} {HELP}},
	url = {http://arxiv.org/abs/2311.15112},
	doi = {10.48550/arXiv.2311.15112},
	abstract = {Graph neural networks (GNNs) have led to major breakthroughs in a variety of domains such as drug discovery, social network analysis, and travel time estimation. However, they lack interpretability which hinders human trust and thereby deployment to settings with high-stakes decisions. A line of interpretable methods approach this by discovering a small set of relevant concepts as subgraphs in the last GNN layer that together explain the prediction. This can yield oversimplified explanations, failing to explain the interaction between GNN layers. To address this oversight, we provide HELP (Hierarchical Explainable Latent Pooling), a novel, inherently interpretable graph pooling approach that reveals how concepts from different GNN layers compose to new ones in later steps. HELP is more than 1-WL expressive and is the first non-spectral, end-to-end-learnable, hierarchical graph pooling method that can learn to pool a variable number of arbitrary connected components. We empirically demonstrate that it performs on-par with standard GCNs and popular pooling methods in terms of accuracy while yielding explanations that are aligned with expert knowledge in the domains of chemistry and social networks. In addition to a qualitative analysis, we employ concept completeness scores as well as concept conformity, a novel metric to measure the noise in discovered concepts, quantitatively verifying that the discovered concepts are significantly easier to fully understand than those from previous work. Our work represents a first step towards an understanding of graph neural networks that goes beyond a set of concepts from the final layer and instead explains the complex interplay of concepts on different levels.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Jürß, Jonas and Magister, Lucie Charlotte and Barbiero, Pietro and Liò, Pietro and Simidjievski, Nikola},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lewisOneBiggestProblems2023,
	title = {One of the {Biggest} {Problems} in {Biology} {Has} {Finally} {Been} {Solved}},
	url = {https://www.scientificamerican.com/article/one-of-the-biggest-problems-in-biology-has-finally-been-solved/},
	abstract = {Google DeepMind CEO Demis Hassabis explains how its AlphaFold AI program predicted the 3-D structure of every known protein},
	language = {en},
	urldate = {2023-11-27},
	journal = {Scientific American},
	author = {Lewis, Tanya},
	month = feb,
	year = {2023},
}

@article{jumperAlphaFold2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2023-11-24},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
}

@misc{schutBridgingHumanAIKnowledge2023,
	title = {Bridging the {Human}-{AI} {Knowledge} {Gap}: {Concept} {Discovery} and {Transfer} in {AlphaZero}},
	shorttitle = {Bridging the {Human}-{AI} {Knowledge} {Gap}},
	url = {http://arxiv.org/abs/2310.16410},
	abstract = {Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Schut, Lisa and Tomasev, Nenad and McGrath, Tom and Hassabis, Demis and Paquet, Ulrich and Kim, Been},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16410 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@misc{holtgenDeDUCEGeneratingCounterfactual2021,
	title = {{DeDUCE}: {Generating} {Counterfactual} {Explanations} {Efficiently}},
	shorttitle = {{DeDUCE}},
	url = {http://arxiv.org/abs/2111.15639},
	doi = {10.48550/arXiv.2111.15639},
	abstract = {When an image classifier outputs a wrong class label, it can be helpful to see what changes in the image would lead to a correct classification. This is the aim of algorithms generating counterfactual explanations. However, there is no easily scalable method to generate such counterfactuals. We develop a new algorithm providing counterfactual explanations for large image classifiers trained with spectral normalisation at low computational cost. We empirically compare this algorithm against baselines from the literature; our novel algorithm consistently finds counterfactuals that are much closer to the original inputs. At the same time, the realism of these counterfactuals is comparable to the baselines. The code for all experiments is available at https://github.com/benedikthoeltgen/DeDUCE.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Höltgen, Benedikt and Schut, Lisa and Brauner, Jan M. and Gal, Yarin},
	month = nov,
	year = {2021},
	note = {arXiv:2111.15639 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@inproceedings{kanBrainNetworkTransformer2022,
	title = {Brain {Network} {Transformer}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/a408234a9b80604a9cf6ca518e474550-Abstract-Conference.html},
	booktitle = {{NeurIPS}},
	author = {Kan, Xuan and Dai, Wei and Cui, Hejie and Zhang, Zilong and Guo, Ying and Yang, Carl},
	year = {2022},
}

@inproceedings{nakkiranDeepDoubleDescent2020,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	url = {https://openreview.net/forum?id=B1g5sA4twr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	year = {2020},
}

@misc{powerGrokkingGeneralizationOverfitting2022,
	title = {Grokking: {Generalization} {Beyond} {Overfitting} on {Small} {Algorithmic} {Datasets}},
	shorttitle = {Grokking},
	url = {http://arxiv.org/abs/2201.02177},
	doi = {10.48550/arXiv.2201.02177},
	abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
	urldate = {2023-11-12},
	publisher = {arXiv},
	author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02177 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{footeNeuronGraphInterpreting2023,
	title = {Neuron to {Graph}: {Interpreting} {Language} {Model} {Neurons} at {Scale}},
	shorttitle = {Neuron to {Graph}},
	url = {http://arxiv.org/abs/2305.19911},
	doi = {10.48550/arXiv.2305.19911},
	abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
	month = may,
	year = {2023},
	note = {arXiv:2305.19911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{giovanniOverSquashingMessagePassing2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}: {The} {Impact} of {Width}, {Depth}, and {Topology}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/di-giovanni23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Giovanni, Francesco Di and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio, Pietro and Bronstein, Michael M.},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {7865--7885},
}

@article{finkelshteinSinglenodeAttacksFooling2022,
	title = {Single-node attacks for fooling graph neural networks},
	volume = {513},
	url = {https://doi.org/10.1016/j.neucom.2022.09.115},
	doi = {10.1016/J.NEUCOM.2022.09.115},
	journal = {Neurocomputing},
	author = {Finkelshtein, Ben and Baskin, Chaim and Zheltonozhskii, Evgenii and Alon, Uri},
	year = {2022},
	keywords = {notion},
	pages = {1--12},
}

@misc{lanLocatingCrossTaskSequence2023,
	title = {Locating {Cross}-{Task} {Sequence} {Continuation} {Circuits} in {Transformers}},
	url = {http://arxiv.org/abs/2311.04131},
	doi = {10.48550/arXiv.2311.04131},
	abstract = {While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Lan, Michael and Barez, Fazl},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{gevaTransformerFeedForwardLayers2022,
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {https://doi.org/10.18653/v1/2022.emnlp-main.3},
	doi = {10.18653/V1/2022.EMNLP-MAIN.3},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2022, {Abu} {Dhabi}, {United} {Arab} {Emirates}, {December} 7-11, 2022},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	year = {2022},
	keywords = {notion},
	pages = {30--45},
}

@inproceedings{gevaTransformerFeedForwardLayers2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {https://doi.org/10.18653/v1/2021.emnlp-main.446},
	doi = {10.18653/V1/2021.EMNLP-MAIN.446},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2021, {Virtual} {Event} / {Punta} {Cana}, {Dominican} {Republic}, 7-11 {November}, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	year = {2021},
	keywords = {notion},
	pages = {5484--5495},
}

@inproceedings{chughtaiToyModelUniversality2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Toy} {Model} of {Universality}: {Reverse} {Engineering} how {Networks} {Learn} {Group} {Operations}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/chughtai23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {6243--6267},
}

@inproceedings{krizhevskyAlexNet2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
	year = {2012},
}

@article{olahNaturallyOccurringEquivariance2020,
	title = {Naturally {Occurring} {Equivariance} in {Neural} {Networks}},
	url = {https://distill.pub/2020/circuits/equivariance},
	doi = {10.23915/distill.00024.004},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Voss, Chelsea and Schubert, Ludwig and Goh, Gabriel},
	year = {2020},
	keywords = {notion},
}

@article{cammarataCurveDetectors2020,
	title = {Curve {Detectors}},
	url = {https://distill.pub/2020/circuits/curve-detectors},
	doi = {10.23915/distill.00024.003},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
	year = {2020},
}

@article{olahZoomIntroductionCircuits2020,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	url = {https://distill.pub/2020/circuits/zoom-in},
	doi = {10.23915/distill.00024.001},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
	year = {2020},
	keywords = {notion},
}

@article{petrovWeightBanding2021,
	title = {Weight {Banding}},
	url = {https://distill.pub/2020/circuits/weight-banding},
	doi = {10.23915/distill.00024.009},
	journal = {Distill},
	author = {Petrov, Michael and Voss, Chelsea and Schubert, Ludwig and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{yangHowGNNsLearn2023,
	title = {How {Graph} {Neural} {Networks} {Learn}: {Lessons} from {Training} {Dynamics} in {Function} {Space}},
	shorttitle = {How {Graph} {Neural} {Networks} {Learn}},
	url = {http://arxiv.org/abs/2310.05105},
	abstract = {A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, however it remains less clear whether and how GNNs learn desired functions during the optimization process. To fill this critical gap, we study the learning dynamics of GNNs in function space via the analytic framework of overparameterization. In particular, we find that the seemingly complicated training process of GNNs can be re-cast into a more familiar label propagation framework, due to the graph inductive bias implicit in this process. From this vantage point, we provide explanations for why the learned GNN functions successfully generalize and for their pathological behavior on heterophilic graphs, which are consistent with observations. Practically, sparsifying and implementing the learning dynamics lead to a minimalist semi-supervised learning algorithm with the efficiency of classic algorithms and the effectiveness of modern GNNs.},
	urldate = {2023-10-30},
	publisher = {arXiv},
	author = {Yang, Chenxiao and Wu, Qitian and Wipf, David and Sun, Ruoyu and Yan, Junchi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05105 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
}

@misc{kipfVariationalGraphAutoEncoders2016,
	title = {Variational {Graph} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1611.07308},
	doi = {10.48550/arXiv.1611.07308},
	abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = nov,
	year = {2016},
	note = {arXiv:1611.07308 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@article{olahMechanisticInterpretabilityVariables2022,
	title = {Mechanistic {Interpretability}, {Variables}, and the {Importance} of {Interpretable} {Bases}},
	url = {https://transformer-circuits.pub/2022/mech-interp-essay/},
	journal = {Transformer Circuits Thread},
	author = {Olah, Chris},
	year = {2022},
}

@article{olahFeatureVisualization2017,
	title = {Feature {Visualization}},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	year = {2017},
}

@article{johnsonExtensionsLipschitzMappings1984,
	title = {Extensions of {Lipschitz} mappings into {Hilbert} space},
	volume = {26},
	url = {https://stanford.edu/class/cs114/readings/JL-Johnson.pdf},
	journal = {Contemporary mathematics},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	year = {1984},
	pages = {189--206},
}

@article{aroraLinearAlgebraicStructure2018,
	title = {Linear {Algebraic} {Structure} of {Word} {Senses}, with {Applications} to {Polysemy}},
	volume = {6},
	url = {https://doi.org/10.1162/tacl_a_00034},
	doi = {10.1162/tacl\_a\_00034},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	year = {2018},
	keywords = {notion},
	pages = {483--495},
}

@article{heimersheimCircuitPythonDocstrings2023,
	title = {A circuit for {Python} docstrings in a 4-layer attention-only transformer},
	url = {https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only},
	author = {Heimersheim, Stefan and Janiak, Jett},
	year = {2023},
	keywords = {notion},
}

@article{schubertHighLowFrequencyDetectors2021,
	title = {High-{Low} {Frequency} {Detectors}},
	url = {https://distill.pub/2020/circuits/frequency-edges},
	doi = {10.23915/distill.00024.005},
	journal = {Distill},
	author = {Schubert, Ludwig and Voss, Chelsea and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{cammarataCurveCircuits2021,
	title = {Curve {Circuits}},
	url = {https://distill.pub/2020/circuits/curve-circuits},
	doi = {10.23915/distill.00024.006},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Voss, Chelsea and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{vossBranchSpecialization2021,
	title = {Branch {Specialization}},
	url = {https://distill.pub/2020/circuits/branch-specialization},
	doi = {10.23915/distill.00024.008},
	journal = {Distill},
	author = {Voss, Chelsea and Goh, Gabriel and Cammarata, Nick and Petrov, Michael and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{lindnerTracrCompiledTransformers2023,
	title = {Tracr: {Compiled} {Transformers} as a {Laboratory} for {Interpretability}},
	shorttitle = {Tracr},
	url = {http://arxiv.org/abs/2301.05062},
	abstract = {We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Lindner, David and Kramár, János and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Thomas and Mikulik, Vladimir},
	month = jun,
	year = {2023},
	note = {arXiv:2301.05062 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}


@inproceedings{mengLocatingEditingROME2022,
	title = {Locating and {Editing} {Factual} {Associations} in {GPT}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 35: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2022, {NeurIPS} 2022, {New} {Orleans}, {LA}, {USA}, {November} 28 - {December} 9, 2022},
	author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
	editor = {Koyejo, Sanmi and Mohamed, S. and Agarwal, A. and Belgrave, Danielle and Cho, K. and Oh, A.},
	year = {2022},
	keywords = {notion},
}

@article{huChatGPTSetsRecord2023,
	chapter = {Technology},
	title = {{ChatGPT} sets record for fastest-growing user base - analyst note},
	url = {https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/},
	abstract = {ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after launch, making it the fastest-growing consumer application in history, according to a UBS study on Wednesday.},
	language = {en},
	urldate = {2024-01-14},
	journal = {Reuters},
	author = {Hu, Krystal},
	month = feb,
	year = {2023},
}

@inproceedings{juerssRecursiveAlgorithmicReasoning2023Openreview,
	title = {Recursive {Algorithmic} {Reasoning}},
	url = {https://openreview.net/forum?id=43M1bPorxU},
	booktitle = {The {Second} {Learning} on {Graphs} {Conference}},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja and Veličković, Petar},
	year = {2023},
}

@inproceedings{juerssRecursiveReasoningNeural2023,
	title = {Recursive {Reasoning} with {Neural} {Networks}},
	url = {https://openreview.net/pdf?id=TS8l4VS7_BK},
	booktitle = {The {First} {Tiny} {Papers} {Track} at {ICLR} 2023, {Tiny} {Papers} @ {ICLR} 2023, {Kigali}, {Rwanda}, {May} 5, 2023},
	publisher = {OpenReview.net},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja},
	editor = {Maughan, Krystal and Liu, Rosanne and Burns, Thomas F.},
	year = {2023},
}

@inproceedings{zintgrafVariBADVeryGood2020,
	title = {{VariBAD}: {A} {Very} {Good} {Method} for {Bayes}-{Adaptive} {Deep} {RL} via {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=Hkl9JlBYvr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Zintgraf, Luisa M. and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
	year = {2020},
}

@misc{guMambaLinearTimeSequence2023,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
}

@inproceedings{yasunagaQAGNNReasoningLanguage2021,
	title = {{QA}-{GNN}: {Reasoning} with {Language} {Models} and {Knowledge} {Graphs} for {Question} {Answering}},
	url = {https://doi.org/10.18653/v1/2021.naacl-main.45},
	doi = {10.18653/V1/2021.NAACL-MAIN.45},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2021, {Online}, {June} 6-11, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tür, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	year = {2021},
	keywords = {notion},
	pages = {535--546},
}

@inproceedings{mengGNNLMLanguageModeling2021,
	title = {{GNN}-{LM}: {Language} {Modeling} based on {Global} {Contexts} via {GNN}},
	shorttitle = {{GNN}-{LM}},
	url = {https://openreview.net/forum?id=BS49l-B5Bql},
	abstract = {Inspired by the notion that "it to copy is easier than to memorize", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.},
	language = {en},
	urldate = {2023-12-02},
	author = {Meng, Yuxian and Zong, Shi and Li, Xiaoya and Sun, Xiaofei and Zhang, Tianwei and Wu, Fei and Li, Jiwei},
	month = oct,
	year = {2021},
}

@misc{juerssHELP2023,
	title = {Everybody {Needs} a {Little} {HELP}: {Explaining} {Graphs} via {Hierarchical} {Concepts}},
	shorttitle = {Everybody {Needs} a {Little} {HELP}},
	url = {http://arxiv.org/abs/2311.15112},
	doi = {10.48550/arXiv.2311.15112},
	abstract = {Graph neural networks (GNNs) have led to major breakthroughs in a variety of domains such as drug discovery, social network analysis, and travel time estimation. However, they lack interpretability which hinders human trust and thereby deployment to settings with high-stakes decisions. A line of interpretable methods approach this by discovering a small set of relevant concepts as subgraphs in the last GNN layer that together explain the prediction. This can yield oversimplified explanations, failing to explain the interaction between GNN layers. To address this oversight, we provide HELP (Hierarchical Explainable Latent Pooling), a novel, inherently interpretable graph pooling approach that reveals how concepts from different GNN layers compose to new ones in later steps. HELP is more than 1-WL expressive and is the first non-spectral, end-to-end-learnable, hierarchical graph pooling method that can learn to pool a variable number of arbitrary connected components. We empirically demonstrate that it performs on-par with standard GCNs and popular pooling methods in terms of accuracy while yielding explanations that are aligned with expert knowledge in the domains of chemistry and social networks. In addition to a qualitative analysis, we employ concept completeness scores as well as concept conformity, a novel metric to measure the noise in discovered concepts, quantitatively verifying that the discovered concepts are significantly easier to fully understand than those from previous work. Our work represents a first step towards an understanding of graph neural networks that goes beyond a set of concepts from the final layer and instead explains the complex interplay of concepts on different levels.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Jürß, Jonas and Magister, Lucie Charlotte and Barbiero, Pietro and Liò, Pietro and Simidjievski, Nikola},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lewisOneBiggestProblems2023,
	title = {One of the {Biggest} {Problems} in {Biology} {Has} {Finally} {Been} {Solved}},
	url = {https://www.scientificamerican.com/article/one-of-the-biggest-problems-in-biology-has-finally-been-solved/},
	abstract = {Google DeepMind CEO Demis Hassabis explains how its AlphaFold AI program predicted the 3-D structure of every known protein},
	language = {en},
	urldate = {2023-11-27},
	journal = {Scientific American},
	author = {Lewis, Tanya},
	month = feb,
	year = {2023},
}

@article{jumperAlphaFold2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2023-11-24},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
}

@misc{schutBridgingHumanAIKnowledge2023,
	title = {Bridging the {Human}-{AI} {Knowledge} {Gap}: {Concept} {Discovery} and {Transfer} in {AlphaZero}},
	shorttitle = {Bridging the {Human}-{AI} {Knowledge} {Gap}},
	url = {http://arxiv.org/abs/2310.16410},
	abstract = {Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Schut, Lisa and Tomasev, Nenad and McGrath, Tom and Hassabis, Demis and Paquet, Ulrich and Kim, Been},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16410 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@misc{holtgenDeDUCEGeneratingCounterfactual2021,
	title = {{DeDUCE}: {Generating} {Counterfactual} {Explanations} {Efficiently}},
	shorttitle = {{DeDUCE}},
	url = {http://arxiv.org/abs/2111.15639},
	doi = {10.48550/arXiv.2111.15639},
	abstract = {When an image classifier outputs a wrong class label, it can be helpful to see what changes in the image would lead to a correct classification. This is the aim of algorithms generating counterfactual explanations. However, there is no easily scalable method to generate such counterfactuals. We develop a new algorithm providing counterfactual explanations for large image classifiers trained with spectral normalisation at low computational cost. We empirically compare this algorithm against baselines from the literature; our novel algorithm consistently finds counterfactuals that are much closer to the original inputs. At the same time, the realism of these counterfactuals is comparable to the baselines. The code for all experiments is available at https://github.com/benedikthoeltgen/DeDUCE.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Höltgen, Benedikt and Schut, Lisa and Brauner, Jan M. and Gal, Yarin},
	month = nov,
	year = {2021},
	note = {arXiv:2111.15639 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@inproceedings{kanBrainNetworkTransformer2022,
	title = {Brain {Network} {Transformer}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/a408234a9b80604a9cf6ca518e474550-Abstract-Conference.html},
	booktitle = {{NeurIPS}},
	author = {Kan, Xuan and Dai, Wei and Cui, Hejie and Zhang, Zilong and Guo, Ying and Yang, Carl},
	year = {2022},
}

@inproceedings{nakkiranDeepDoubleDescent2020,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	url = {https://openreview.net/forum?id=B1g5sA4twr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	year = {2020},
}

@misc{powerGrokkingGeneralizationOverfitting2022,
	title = {Grokking: {Generalization} {Beyond} {Overfitting} on {Small} {Algorithmic} {Datasets}},
	shorttitle = {Grokking},
	url = {http://arxiv.org/abs/2201.02177},
	doi = {10.48550/arXiv.2201.02177},
	abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
	urldate = {2023-11-12},
	publisher = {arXiv},
	author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02177 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{footeNeuronGraphInterpreting2023,
	title = {Neuron to {Graph}: {Interpreting} {Language} {Model} {Neurons} at {Scale}},
	shorttitle = {Neuron to {Graph}},
	url = {http://arxiv.org/abs/2305.19911},
	doi = {10.48550/arXiv.2305.19911},
	abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
	month = may,
	year = {2023},
	note = {arXiv:2305.19911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{giovanniOverSquashingMessagePassing2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}: {The} {Impact} of {Width}, {Depth}, and {Topology}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/di-giovanni23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Giovanni, Francesco Di and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio, Pietro and Bronstein, Michael M.},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {7865--7885},
}

@article{finkelshteinSinglenodeAttacksFooling2022,
	title = {Single-node attacks for fooling graph neural networks},
	volume = {513},
	url = {https://doi.org/10.1016/j.neucom.2022.09.115},
	doi = {10.1016/J.NEUCOM.2022.09.115},
	journal = {Neurocomputing},
	author = {Finkelshtein, Ben and Baskin, Chaim and Zheltonozhskii, Evgenii and Alon, Uri},
	year = {2022},
	keywords = {notion},
	pages = {1--12},
}

@misc{lanLocatingCrossTaskSequence2023,
	title = {Locating {Cross}-{Task} {Sequence} {Continuation} {Circuits} in {Transformers}},
	url = {http://arxiv.org/abs/2311.04131},
	doi = {10.48550/arXiv.2311.04131},
	abstract = {While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Lan, Michael and Barez, Fazl},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{gevaTransformerFeedForwardLayers2022,
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {https://doi.org/10.18653/v1/2022.emnlp-main.3},
	doi = {10.18653/V1/2022.EMNLP-MAIN.3},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2022, {Abu} {Dhabi}, {United} {Arab} {Emirates}, {December} 7-11, 2022},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	year = {2022},
	keywords = {notion},
	pages = {30--45},
}

@inproceedings{gevaTransformerFeedForwardLayers2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {https://doi.org/10.18653/v1/2021.emnlp-main.446},
	doi = {10.18653/V1/2021.EMNLP-MAIN.446},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2021, {Virtual} {Event} / {Punta} {Cana}, {Dominican} {Republic}, 7-11 {November}, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	year = {2021},
	keywords = {notion},
	pages = {5484--5495},
}

@inproceedings{chughtaiToyModelUniversality2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Toy} {Model} of {Universality}: {Reverse} {Engineering} how {Networks} {Learn} {Group} {Operations}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/chughtai23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {6243--6267},
}

@inproceedings{krizhevskyAlexNet2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
	year = {2012},
}

@article{olahNaturallyOccurringEquivariance2020,
	title = {Naturally {Occurring} {Equivariance} in {Neural} {Networks}},
	url = {https://distill.pub/2020/circuits/equivariance},
	doi = {10.23915/distill.00024.004},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Voss, Chelsea and Schubert, Ludwig and Goh, Gabriel},
	year = {2020},
	keywords = {notion},
}

@article{cammarataCurveDetectors2020,
	title = {Curve {Detectors}},
	url = {https://distill.pub/2020/circuits/curve-detectors},
	doi = {10.23915/distill.00024.003},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
	year = {2020},
}

@article{olahZoomIntroductionCircuits2020,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	url = {https://distill.pub/2020/circuits/zoom-in},
	doi = {10.23915/distill.00024.001},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
	year = {2020},
	keywords = {notion},
}

@article{petrovWeightBanding2021,
	title = {Weight {Banding}},
	url = {https://distill.pub/2020/circuits/weight-banding},
	doi = {10.23915/distill.00024.009},
	journal = {Distill},
	author = {Petrov, Michael and Voss, Chelsea and Schubert, Ludwig and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{yangHowGNNsLearn2023,
	title = {How {Graph} {Neural} {Networks} {Learn}: {Lessons} from {Training} {Dynamics} in {Function} {Space}},
	shorttitle = {How {Graph} {Neural} {Networks} {Learn}},
	url = {http://arxiv.org/abs/2310.05105},
	abstract = {A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, however it remains less clear whether and how GNNs learn desired functions during the optimization process. To fill this critical gap, we study the learning dynamics of GNNs in function space via the analytic framework of overparameterization. In particular, we find that the seemingly complicated training process of GNNs can be re-cast into a more familiar label propagation framework, due to the graph inductive bias implicit in this process. From this vantage point, we provide explanations for why the learned GNN functions successfully generalize and for their pathological behavior on heterophilic graphs, which are consistent with observations. Practically, sparsifying and implementing the learning dynamics lead to a minimalist semi-supervised learning algorithm with the efficiency of classic algorithms and the effectiveness of modern GNNs.},
	urldate = {2023-10-30},
	publisher = {arXiv},
	author = {Yang, Chenxiao and Wu, Qitian and Wipf, David and Sun, Ruoyu and Yan, Junchi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05105 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
}

@misc{kipfVariationalGraphAutoEncoders2016,
	title = {Variational {Graph} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1611.07308},
	doi = {10.48550/arXiv.1611.07308},
	abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = nov,
	year = {2016},
	note = {arXiv:1611.07308 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@article{olahMechanisticInterpretabilityVariables2022,
	title = {Mechanistic {Interpretability}, {Variables}, and the {Importance} of {Interpretable} {Bases}},
	url = {https://transformer-circuits.pub/2022/mech-interp-essay/},
	journal = {Transformer Circuits Thread},
	author = {Olah, Chris},
	year = {2022},
}

@article{olahFeatureVisualization2017,
	title = {Feature {Visualization}},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	year = {2017},
}

@article{johnsonExtensionsLipschitzMappings1984,
	title = {Extensions of {Lipschitz} mappings into {Hilbert} space},
	volume = {26},
	url = {https://stanford.edu/class/cs114/readings/JL-Johnson.pdf},
	journal = {Contemporary mathematics},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	year = {1984},
	pages = {189--206},
}

@article{aroraLinearAlgebraicStructure2018,
	title = {Linear {Algebraic} {Structure} of {Word} {Senses}, with {Applications} to {Polysemy}},
	volume = {6},
	url = {https://doi.org/10.1162/tacl_a_00034},
	doi = {10.1162/tacl\_a\_00034},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	year = {2018},
	keywords = {notion},
	pages = {483--495},
}

@article{heimersheimCircuitPythonDocstrings2023,
	title = {A circuit for {Python} docstrings in a 4-layer attention-only transformer},
	url = {https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only},
	author = {Heimersheim, Stefan and Janiak, Jett},
	year = {2023},
	keywords = {notion},
}

@article{schubertHighLowFrequencyDetectors2021,
	title = {High-{Low} {Frequency} {Detectors}},
	url = {https://distill.pub/2020/circuits/frequency-edges},
	doi = {10.23915/distill.00024.005},
	journal = {Distill},
	author = {Schubert, Ludwig and Voss, Chelsea and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{cammarataCurveCircuits2021,
	title = {Curve {Circuits}},
	url = {https://distill.pub/2020/circuits/curve-circuits},
	doi = {10.23915/distill.00024.006},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Voss, Chelsea and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{vossBranchSpecialization2021,
	title = {Branch {Specialization}},
	url = {https://distill.pub/2020/circuits/branch-specialization},
	doi = {10.23915/distill.00024.008},
	journal = {Distill},
	author = {Voss, Chelsea and Goh, Gabriel and Cammarata, Nick and Petrov, Michael and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{lindnerTracrCompiledTransformers2023,
	title = {Tracr: {Compiled} {Transformers} as a {Laboratory} for {Interpretability}},
	shorttitle = {Tracr},
	url = {http://arxiv.org/abs/2301.05062},
	abstract = {We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Lindner, David and Kramár, János and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Thomas and Mikulik, Vladimir},
	month = jun,
	year = {2023},
	note = {arXiv:2301.05062 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}


@article{huChatGPTSetsRecord2023,
	chapter = {Technology},
	title = {{ChatGPT} sets record for fastest-growing user base - analyst note},
	url = {https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/},
	abstract = {ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after launch, making it the fastest-growing consumer application in history, according to a UBS study on Wednesday.},
	language = {en},
	urldate = {2024-01-14},
	journal = {Reuters},
	author = {Hu, Krystal},
	month = feb,
	year = {2023},
}

@inproceedings{juerssRecursiveAlgorithmicReasoning2023Openreview,
	title = {Recursive {Algorithmic} {Reasoning}},
	url = {https://openreview.net/forum?id=43M1bPorxU},
	booktitle = {The {Second} {Learning} on {Graphs} {Conference}},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja and Veličković, Petar},
	year = {2023},
}

@inproceedings{juerssRecursiveReasoningNeural2023,
	title = {Recursive {Reasoning} with {Neural} {Networks}},
	url = {https://openreview.net/pdf?id=TS8l4VS7_BK},
	booktitle = {The {First} {Tiny} {Papers} {Track} at {ICLR} 2023, {Tiny} {Papers} @ {ICLR} 2023, {Kigali}, {Rwanda}, {May} 5, 2023},
	publisher = {OpenReview.net},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja},
	editor = {Maughan, Krystal and Liu, Rosanne and Burns, Thomas F.},
	year = {2023},
}

@inproceedings{zintgrafVariBADVeryGood2020,
	title = {{VariBAD}: {A} {Very} {Good} {Method} for {Bayes}-{Adaptive} {Deep} {RL} via {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=Hkl9JlBYvr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Zintgraf, Luisa M. and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
	year = {2020},
}

@misc{guMambaLinearTimeSequence2023,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
}

@inproceedings{yasunagaQAGNNReasoningLanguage2021,
	title = {{QA}-{GNN}: {Reasoning} with {Language} {Models} and {Knowledge} {Graphs} for {Question} {Answering}},
	url = {https://doi.org/10.18653/v1/2021.naacl-main.45},
	doi = {10.18653/V1/2021.NAACL-MAIN.45},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2021, {Online}, {June} 6-11, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tür, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	year = {2021},
	keywords = {notion},
	pages = {535--546},
}

@inproceedings{mengGNNLMLanguageModeling2021,
	title = {{GNN}-{LM}: {Language} {Modeling} based on {Global} {Contexts} via {GNN}},
	shorttitle = {{GNN}-{LM}},
	url = {https://openreview.net/forum?id=BS49l-B5Bql},
	abstract = {Inspired by the notion that "it to copy is easier than to memorize", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.},
	language = {en},
	urldate = {2023-12-02},
	author = {Meng, Yuxian and Zong, Shi and Li, Xiaoya and Sun, Xiaofei and Zhang, Tianwei and Wu, Fei and Li, Jiwei},
	month = oct,
	year = {2021},
}

@misc{juerssHELP2023,
	title = {Everybody {Needs} a {Little} {HELP}: {Explaining} {Graphs} via {Hierarchical} {Concepts}},
	shorttitle = {Everybody {Needs} a {Little} {HELP}},
	url = {http://arxiv.org/abs/2311.15112},
	doi = {10.48550/arXiv.2311.15112},
	abstract = {Graph neural networks (GNNs) have led to major breakthroughs in a variety of domains such as drug discovery, social network analysis, and travel time estimation. However, they lack interpretability which hinders human trust and thereby deployment to settings with high-stakes decisions. A line of interpretable methods approach this by discovering a small set of relevant concepts as subgraphs in the last GNN layer that together explain the prediction. This can yield oversimplified explanations, failing to explain the interaction between GNN layers. To address this oversight, we provide HELP (Hierarchical Explainable Latent Pooling), a novel, inherently interpretable graph pooling approach that reveals how concepts from different GNN layers compose to new ones in later steps. HELP is more than 1-WL expressive and is the first non-spectral, end-to-end-learnable, hierarchical graph pooling method that can learn to pool a variable number of arbitrary connected components. We empirically demonstrate that it performs on-par with standard GCNs and popular pooling methods in terms of accuracy while yielding explanations that are aligned with expert knowledge in the domains of chemistry and social networks. In addition to a qualitative analysis, we employ concept completeness scores as well as concept conformity, a novel metric to measure the noise in discovered concepts, quantitatively verifying that the discovered concepts are significantly easier to fully understand than those from previous work. Our work represents a first step towards an understanding of graph neural networks that goes beyond a set of concepts from the final layer and instead explains the complex interplay of concepts on different levels.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Jürß, Jonas and Magister, Lucie Charlotte and Barbiero, Pietro and Liò, Pietro and Simidjievski, Nikola},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lewisOneBiggestProblems2023,
	title = {One of the {Biggest} {Problems} in {Biology} {Has} {Finally} {Been} {Solved}},
	url = {https://www.scientificamerican.com/article/one-of-the-biggest-problems-in-biology-has-finally-been-solved/},
	abstract = {Google DeepMind CEO Demis Hassabis explains how its AlphaFold AI program predicted the 3-D structure of every known protein},
	language = {en},
	urldate = {2023-11-27},
	journal = {Scientific American},
	author = {Lewis, Tanya},
	month = feb,
	year = {2023},
}

@article{jumperAlphaFold2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2023-11-24},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
}

@misc{schutBridgingHumanAIKnowledge2023,
	title = {Bridging the {Human}-{AI} {Knowledge} {Gap}: {Concept} {Discovery} and {Transfer} in {AlphaZero}},
	shorttitle = {Bridging the {Human}-{AI} {Knowledge} {Gap}},
	url = {http://arxiv.org/abs/2310.16410},
	abstract = {Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Schut, Lisa and Tomasev, Nenad and McGrath, Tom and Hassabis, Demis and Paquet, Ulrich and Kim, Been},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16410 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@misc{holtgenDeDUCEGeneratingCounterfactual2021,
	title = {{DeDUCE}: {Generating} {Counterfactual} {Explanations} {Efficiently}},
	shorttitle = {{DeDUCE}},
	url = {http://arxiv.org/abs/2111.15639},
	doi = {10.48550/arXiv.2111.15639},
	abstract = {When an image classifier outputs a wrong class label, it can be helpful to see what changes in the image would lead to a correct classification. This is the aim of algorithms generating counterfactual explanations. However, there is no easily scalable method to generate such counterfactuals. We develop a new algorithm providing counterfactual explanations for large image classifiers trained with spectral normalisation at low computational cost. We empirically compare this algorithm against baselines from the literature; our novel algorithm consistently finds counterfactuals that are much closer to the original inputs. At the same time, the realism of these counterfactuals is comparable to the baselines. The code for all experiments is available at https://github.com/benedikthoeltgen/DeDUCE.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Höltgen, Benedikt and Schut, Lisa and Brauner, Jan M. and Gal, Yarin},
	month = nov,
	year = {2021},
	note = {arXiv:2111.15639 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@inproceedings{kanBrainNetworkTransformer2022,
	title = {Brain {Network} {Transformer}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/a408234a9b80604a9cf6ca518e474550-Abstract-Conference.html},
	booktitle = {{NeurIPS}},
	author = {Kan, Xuan and Dai, Wei and Cui, Hejie and Zhang, Zilong and Guo, Ying and Yang, Carl},
	year = {2022},
}

@inproceedings{nakkiranDeepDoubleDescent2020,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	url = {https://openreview.net/forum?id=B1g5sA4twr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	year = {2020},
}

@misc{powerGrokkingGeneralizationOverfitting2022,
	title = {Grokking: {Generalization} {Beyond} {Overfitting} on {Small} {Algorithmic} {Datasets}},
	shorttitle = {Grokking},
	url = {http://arxiv.org/abs/2201.02177},
	doi = {10.48550/arXiv.2201.02177},
	abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
	urldate = {2023-11-12},
	publisher = {arXiv},
	author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02177 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{footeNeuronGraphInterpreting2023,
	title = {Neuron to {Graph}: {Interpreting} {Language} {Model} {Neurons} at {Scale}},
	shorttitle = {Neuron to {Graph}},
	url = {http://arxiv.org/abs/2305.19911},
	doi = {10.48550/arXiv.2305.19911},
	abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
	month = may,
	year = {2023},
	note = {arXiv:2305.19911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{giovanniOverSquashingMessagePassing2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}: {The} {Impact} of {Width}, {Depth}, and {Topology}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/di-giovanni23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Giovanni, Francesco Di and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio, Pietro and Bronstein, Michael M.},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {7865--7885},
}

@article{finkelshteinSinglenodeAttacksFooling2022,
	title = {Single-node attacks for fooling graph neural networks},
	volume = {513},
	url = {https://doi.org/10.1016/j.neucom.2022.09.115},
	doi = {10.1016/J.NEUCOM.2022.09.115},
	journal = {Neurocomputing},
	author = {Finkelshtein, Ben and Baskin, Chaim and Zheltonozhskii, Evgenii and Alon, Uri},
	year = {2022},
	keywords = {notion},
	pages = {1--12},
}

@misc{lanLocatingCrossTaskSequence2023,
	title = {Locating {Cross}-{Task} {Sequence} {Continuation} {Circuits} in {Transformers}},
	url = {http://arxiv.org/abs/2311.04131},
	doi = {10.48550/arXiv.2311.04131},
	abstract = {While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Lan, Michael and Barez, Fazl},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{gevaTransformerFeedForwardLayers2022,
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {https://doi.org/10.18653/v1/2022.emnlp-main.3},
	doi = {10.18653/V1/2022.EMNLP-MAIN.3},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2022, {Abu} {Dhabi}, {United} {Arab} {Emirates}, {December} 7-11, 2022},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	year = {2022},
	keywords = {notion},
	pages = {30--45},
}

@inproceedings{gevaTransformerFeedForwardLayers2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {https://doi.org/10.18653/v1/2021.emnlp-main.446},
	doi = {10.18653/V1/2021.EMNLP-MAIN.446},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2021, {Virtual} {Event} / {Punta} {Cana}, {Dominican} {Republic}, 7-11 {November}, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	year = {2021},
	keywords = {notion},
	pages = {5484--5495},
}

@inproceedings{chughtaiToyModelUniversality2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Toy} {Model} of {Universality}: {Reverse} {Engineering} how {Networks} {Learn} {Group} {Operations}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/chughtai23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {6243--6267},
}

@inproceedings{krizhevskyAlexNet2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
	year = {2012},
}

@article{olahNaturallyOccurringEquivariance2020,
	title = {Naturally {Occurring} {Equivariance} in {Neural} {Networks}},
	url = {https://distill.pub/2020/circuits/equivariance},
	doi = {10.23915/distill.00024.004},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Voss, Chelsea and Schubert, Ludwig and Goh, Gabriel},
	year = {2020},
	keywords = {notion},
}

@article{cammarataCurveDetectors2020,
	title = {Curve {Detectors}},
	url = {https://distill.pub/2020/circuits/curve-detectors},
	doi = {10.23915/distill.00024.003},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
	year = {2020},
}

@article{olahZoomIntroductionCircuits2020,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	url = {https://distill.pub/2020/circuits/zoom-in},
	doi = {10.23915/distill.00024.001},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
	year = {2020},
	keywords = {notion},
}

@article{petrovWeightBanding2021,
	title = {Weight {Banding}},
	url = {https://distill.pub/2020/circuits/weight-banding},
	doi = {10.23915/distill.00024.009},
	journal = {Distill},
	author = {Petrov, Michael and Voss, Chelsea and Schubert, Ludwig and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{yangHowGNNsLearn2023,
	title = {How {Graph} {Neural} {Networks} {Learn}: {Lessons} from {Training} {Dynamics} in {Function} {Space}},
	shorttitle = {How {Graph} {Neural} {Networks} {Learn}},
	url = {http://arxiv.org/abs/2310.05105},
	abstract = {A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, however it remains less clear whether and how GNNs learn desired functions during the optimization process. To fill this critical gap, we study the learning dynamics of GNNs in function space via the analytic framework of overparameterization. In particular, we find that the seemingly complicated training process of GNNs can be re-cast into a more familiar label propagation framework, due to the graph inductive bias implicit in this process. From this vantage point, we provide explanations for why the learned GNN functions successfully generalize and for their pathological behavior on heterophilic graphs, which are consistent with observations. Practically, sparsifying and implementing the learning dynamics lead to a minimalist semi-supervised learning algorithm with the efficiency of classic algorithms and the effectiveness of modern GNNs.},
	urldate = {2023-10-30},
	publisher = {arXiv},
	author = {Yang, Chenxiao and Wu, Qitian and Wipf, David and Sun, Ruoyu and Yan, Junchi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05105 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
}

@misc{kipfVariationalGraphAutoEncoders2016,
	title = {Variational {Graph} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1611.07308},
	doi = {10.48550/arXiv.1611.07308},
	abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = nov,
	year = {2016},
	note = {arXiv:1611.07308 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@article{olahMechanisticInterpretabilityVariables2022,
	title = {Mechanistic {Interpretability}, {Variables}, and the {Importance} of {Interpretable} {Bases}},
	url = {https://transformer-circuits.pub/2022/mech-interp-essay/},
	journal = {Transformer Circuits Thread},
	author = {Olah, Chris},
	year = {2022},
}

@article{olahFeatureVisualization2017,
	title = {Feature {Visualization}},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	year = {2017},
}

@article{johnsonExtensionsLipschitzMappings1984,
	title = {Extensions of {Lipschitz} mappings into {Hilbert} space},
	volume = {26},
	url = {https://stanford.edu/class/cs114/readings/JL-Johnson.pdf},
	journal = {Contemporary mathematics},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	year = {1984},
	pages = {189--206},
}

@article{aroraLinearAlgebraicStructure2018,
	title = {Linear {Algebraic} {Structure} of {Word} {Senses}, with {Applications} to {Polysemy}},
	volume = {6},
	url = {https://doi.org/10.1162/tacl_a_00034},
	doi = {10.1162/tacl\_a\_00034},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	year = {2018},
	keywords = {notion},
	pages = {483--495},
}

@article{heimersheimCircuitPythonDocstrings2023,
	title = {A circuit for {Python} docstrings in a 4-layer attention-only transformer},
	url = {https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only},
	author = {Heimersheim, Stefan and Janiak, Jett},
	year = {2023},
	keywords = {notion},
}

@article{schubertHighLowFrequencyDetectors2021,
	title = {High-{Low} {Frequency} {Detectors}},
	url = {https://distill.pub/2020/circuits/frequency-edges},
	doi = {10.23915/distill.00024.005},
	journal = {Distill},
	author = {Schubert, Ludwig and Voss, Chelsea and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{cammarataCurveCircuits2021,
	title = {Curve {Circuits}},
	url = {https://distill.pub/2020/circuits/curve-circuits},
	doi = {10.23915/distill.00024.006},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Voss, Chelsea and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{vossBranchSpecialization2021,
	title = {Branch {Specialization}},
	url = {https://distill.pub/2020/circuits/branch-specialization},
	doi = {10.23915/distill.00024.008},
	journal = {Distill},
	author = {Voss, Chelsea and Goh, Gabriel and Cammarata, Nick and Petrov, Michael and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{lindnerTracrCompiledTransformers2023,
	title = {Tracr: {Compiled} {Transformers} as a {Laboratory} for {Interpretability}},
	shorttitle = {Tracr},
	url = {http://arxiv.org/abs/2301.05062},
	abstract = {We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Lindner, David and Kramár, János and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Thomas and Mikulik, Vladimir},
	month = jun,
	year = {2023},
	note = {arXiv:2301.05062 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}


@article{huChatGPTSetsRecord2023,
	chapter = {Technology},
	title = {{ChatGPT} sets record for fastest-growing user base - analyst note},
	url = {https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/},
	abstract = {ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after launch, making it the fastest-growing consumer application in history, according to a UBS study on Wednesday.},
	language = {en},
	urldate = {2024-01-14},
	journal = {Reuters},
	author = {Hu, Krystal},
	month = feb,
	year = {2023},
}

@inproceedings{juerssRecursiveAlgorithmicReasoning2023Openreview,
	title = {Recursive {Algorithmic} {Reasoning}},
	url = {https://openreview.net/forum?id=43M1bPorxU},
	booktitle = {The {Second} {Learning} on {Graphs} {Conference}},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja and Veličković, Petar},
	year = {2023},
}

@inproceedings{juerssRecursiveReasoningNeural2023,
	title = {Recursive {Reasoning} with {Neural} {Networks}},
	url = {https://openreview.net/pdf?id=TS8l4VS7_BK},
	booktitle = {The {First} {Tiny} {Papers} {Track} at {ICLR} 2023, {Tiny} {Papers} @ {ICLR} 2023, {Kigali}, {Rwanda}, {May} 5, 2023},
	publisher = {OpenReview.net},
	author = {Jürß, Jonas and Jayalath, Dulhan Hansaja},
	editor = {Maughan, Krystal and Liu, Rosanne and Burns, Thomas F.},
	year = {2023},
}

@inproceedings{zintgrafVariBADVeryGood2020,
	title = {{VariBAD}: {A} {Very} {Good} {Method} for {Bayes}-{Adaptive} {Deep} {RL} via {Meta}-{Learning}},
	url = {https://openreview.net/forum?id=Hkl9JlBYvr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Zintgraf, Luisa M. and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
	year = {2020},
}

@misc{guMambaLinearTimeSequence2023,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-01-06},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
}

@inproceedings{yasunagaQAGNNReasoningLanguage2021,
	title = {{QA}-{GNN}: {Reasoning} with {Language} {Models} and {Knowledge} {Graphs} for {Question} {Answering}},
	url = {https://doi.org/10.18653/v1/2021.naacl-main.45},
	doi = {10.18653/V1/2021.NAACL-MAIN.45},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2021, {Online}, {June} 6-11, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tür, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	year = {2021},
	keywords = {notion},
	pages = {535--546},
}

@inproceedings{mengGNNLMLanguageModeling2021,
	title = {{GNN}-{LM}: {Language} {Modeling} based on {Global} {Contexts} via {GNN}},
	shorttitle = {{GNN}-{LM}},
	url = {https://openreview.net/forum?id=BS49l-B5Bql},
	abstract = {Inspired by the notion that "it to copy is easier than to memorize", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.},
	language = {en},
	urldate = {2023-12-02},
	author = {Meng, Yuxian and Zong, Shi and Li, Xiaoya and Sun, Xiaofei and Zhang, Tianwei and Wu, Fei and Li, Jiwei},
	month = oct,
	year = {2021},
}

@misc{juerssHELP2023,
	title = {Everybody {Needs} a {Little} {HELP}: {Explaining} {Graphs} via {Hierarchical} {Concepts}},
	shorttitle = {Everybody {Needs} a {Little} {HELP}},
	url = {http://arxiv.org/abs/2311.15112},
	doi = {10.48550/arXiv.2311.15112},
	abstract = {Graph neural networks (GNNs) have led to major breakthroughs in a variety of domains such as drug discovery, social network analysis, and travel time estimation. However, they lack interpretability which hinders human trust and thereby deployment to settings with high-stakes decisions. A line of interpretable methods approach this by discovering a small set of relevant concepts as subgraphs in the last GNN layer that together explain the prediction. This can yield oversimplified explanations, failing to explain the interaction between GNN layers. To address this oversight, we provide HELP (Hierarchical Explainable Latent Pooling), a novel, inherently interpretable graph pooling approach that reveals how concepts from different GNN layers compose to new ones in later steps. HELP is more than 1-WL expressive and is the first non-spectral, end-to-end-learnable, hierarchical graph pooling method that can learn to pool a variable number of arbitrary connected components. We empirically demonstrate that it performs on-par with standard GCNs and popular pooling methods in terms of accuracy while yielding explanations that are aligned with expert knowledge in the domains of chemistry and social networks. In addition to a qualitative analysis, we employ concept completeness scores as well as concept conformity, a novel metric to measure the noise in discovered concepts, quantitatively verifying that the discovered concepts are significantly easier to fully understand than those from previous work. Our work represents a first step towards an understanding of graph neural networks that goes beyond a set of concepts from the final layer and instead explains the complex interplay of concepts on different levels.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Jürß, Jonas and Magister, Lucie Charlotte and Barbiero, Pietro and Liò, Pietro and Simidjievski, Nikola},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lewisOneBiggestProblems2023,
	title = {One of the {Biggest} {Problems} in {Biology} {Has} {Finally} {Been} {Solved}},
	url = {https://www.scientificamerican.com/article/one-of-the-biggest-problems-in-biology-has-finally-been-solved/},
	abstract = {Google DeepMind CEO Demis Hassabis explains how its AlphaFold AI program predicted the 3-D structure of every known protein},
	language = {en},
	urldate = {2023-11-27},
	journal = {Scientific American},
	author = {Lewis, Tanya},
	month = feb,
	year = {2023},
}

@article{jumperAlphaFold2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2023-11-24},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Number: 7873
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
}

@misc{schutBridgingHumanAIKnowledge2023,
	title = {Bridging the {Human}-{AI} {Knowledge} {Gap}: {Concept} {Discovery} and {Transfer} in {AlphaZero}},
	shorttitle = {Bridging the {Human}-{AI} {Knowledge} {Gap}},
	url = {http://arxiv.org/abs/2310.16410},
	abstract = {Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Schut, Lisa and Tomasev, Nenad and McGrath, Tom and Hassabis, Demis and Paquet, Ulrich and Kim, Been},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16410 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@misc{holtgenDeDUCEGeneratingCounterfactual2021,
	title = {{DeDUCE}: {Generating} {Counterfactual} {Explanations} {Efficiently}},
	shorttitle = {{DeDUCE}},
	url = {http://arxiv.org/abs/2111.15639},
	doi = {10.48550/arXiv.2111.15639},
	abstract = {When an image classifier outputs a wrong class label, it can be helpful to see what changes in the image would lead to a correct classification. This is the aim of algorithms generating counterfactual explanations. However, there is no easily scalable method to generate such counterfactuals. We develop a new algorithm providing counterfactual explanations for large image classifiers trained with spectral normalisation at low computational cost. We empirically compare this algorithm against baselines from the literature; our novel algorithm consistently finds counterfactuals that are much closer to the original inputs. At the same time, the realism of these counterfactuals is comparable to the baselines. The code for all experiments is available at https://github.com/benedikthoeltgen/DeDUCE.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Höltgen, Benedikt and Schut, Lisa and Brauner, Jan M. and Gal, Yarin},
	month = nov,
	year = {2021},
	note = {arXiv:2111.15639 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@inproceedings{kanBrainNetworkTransformer2022,
	title = {Brain {Network} {Transformer}},
	url = {http://papers.nips.cc/paper_files/paper/2022/hash/a408234a9b80604a9cf6ca518e474550-Abstract-Conference.html},
	booktitle = {{NeurIPS}},
	author = {Kan, Xuan and Dai, Wei and Cui, Hejie and Zhang, Zilong and Guo, Ying and Yang, Carl},
	year = {2022},
}

@inproceedings{nakkiranDeepDoubleDescent2020,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	url = {https://openreview.net/forum?id=B1g5sA4twr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	year = {2020},
}

@misc{powerGrokkingGeneralizationOverfitting2022,
	title = {Grokking: {Generalization} {Beyond} {Overfitting} on {Small} {Algorithmic} {Datasets}},
	shorttitle = {Grokking},
	url = {http://arxiv.org/abs/2201.02177},
	doi = {10.48550/arXiv.2201.02177},
	abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
	urldate = {2023-11-12},
	publisher = {arXiv},
	author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02177 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{footeNeuronGraphInterpreting2023,
	title = {Neuron to {Graph}: {Interpreting} {Language} {Model} {Neurons} at {Scale}},
	shorttitle = {Neuron to {Graph}},
	url = {http://arxiv.org/abs/2305.19911},
	doi = {10.48550/arXiv.2305.19911},
	abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
	month = may,
	year = {2023},
	note = {arXiv:2305.19911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{giovanniOverSquashingMessagePassing2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On {Over}-{Squashing} in {Message} {Passing} {Neural} {Networks}: {The} {Impact} of {Width}, {Depth}, and {Topology}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/di-giovanni23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Giovanni, Francesco Di and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio, Pietro and Bronstein, Michael M.},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {7865--7885},
}

@article{finkelshteinSinglenodeAttacksFooling2022,
	title = {Single-node attacks for fooling graph neural networks},
	volume = {513},
	url = {https://doi.org/10.1016/j.neucom.2022.09.115},
	doi = {10.1016/J.NEUCOM.2022.09.115},
	journal = {Neurocomputing},
	author = {Finkelshtein, Ben and Baskin, Chaim and Zheltonozhskii, Evgenii and Alon, Uri},
	year = {2022},
	keywords = {notion},
	pages = {1--12},
}

@misc{lanLocatingCrossTaskSequence2023,
	title = {Locating {Cross}-{Task} {Sequence} {Continuation} {Circuits} in {Transformers}},
	url = {http://arxiv.org/abs/2311.04131},
	doi = {10.48550/arXiv.2311.04131},
	abstract = {While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Lan, Michael and Barez, Fazl},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@inproceedings{gevaTransformerFeedForwardLayers2022,
	title = {Transformer {Feed}-{Forward} {Layers} {Build} {Predictions} by {Promoting} {Concepts} in the {Vocabulary} {Space}},
	url = {https://doi.org/10.18653/v1/2022.emnlp-main.3},
	doi = {10.18653/V1/2022.EMNLP-MAIN.3},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2022, {Abu} {Dhabi}, {United} {Arab} {Emirates}, {December} 7-11, 2022},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	year = {2022},
	keywords = {notion},
	pages = {30--45},
}

@inproceedings{gevaTransformerFeedForwardLayers2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {https://doi.org/10.18653/v1/2021.emnlp-main.446},
	doi = {10.18653/V1/2021.EMNLP-MAIN.446},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2021, {Virtual} {Event} / {Punta} {Cana}, {Dominican} {Republic}, 7-11 {November}, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	year = {2021},
	keywords = {notion},
	pages = {5484--5495},
}

@inproceedings{chughtaiToyModelUniversality2023,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Toy} {Model} of {Universality}: {Reverse} {Engineering} how {Networks} {Learn} {Group} {Operations}},
	volume = {202},
	url = {https://proceedings.mlr.press/v202/chughtai23a.html},
	booktitle = {International {Conference} on {Machine} {Learning}, {ICML} 2023, 23-29 {July} 2023, {Honolulu}, {Hawaii}, {USA}},
	publisher = {PMLR},
	author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
	editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
	year = {2023},
	keywords = {notion},
	pages = {6243--6267},
}

@inproceedings{krizhevskyAlexNet2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
	year = {2012},
}

@article{olahNaturallyOccurringEquivariance2020,
	title = {Naturally {Occurring} {Equivariance} in {Neural} {Networks}},
	url = {https://distill.pub/2020/circuits/equivariance},
	doi = {10.23915/distill.00024.004},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Voss, Chelsea and Schubert, Ludwig and Goh, Gabriel},
	year = {2020},
	keywords = {notion},
}

@article{cammarataCurveDetectors2020,
	title = {Curve {Detectors}},
	url = {https://distill.pub/2020/circuits/curve-detectors},
	doi = {10.23915/distill.00024.003},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
	year = {2020},
}

@article{olahZoomIntroductionCircuits2020,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	url = {https://distill.pub/2020/circuits/zoom-in},
	doi = {10.23915/distill.00024.001},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
	year = {2020},
	keywords = {notion},
}

@article{petrovWeightBanding2021,
	title = {Weight {Banding}},
	url = {https://distill.pub/2020/circuits/weight-banding},
	doi = {10.23915/distill.00024.009},
	journal = {Distill},
	author = {Petrov, Michael and Voss, Chelsea and Schubert, Ludwig and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{yangHowGNNsLearn2023,
	title = {How {Graph} {Neural} {Networks} {Learn}: {Lessons} from {Training} {Dynamics} in {Function} {Space}},
	shorttitle = {How {Graph} {Neural} {Networks} {Learn}},
	url = {http://arxiv.org/abs/2310.05105},
	abstract = {A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, however it remains less clear whether and how GNNs learn desired functions during the optimization process. To fill this critical gap, we study the learning dynamics of GNNs in function space via the analytic framework of overparameterization. In particular, we find that the seemingly complicated training process of GNNs can be re-cast into a more familiar label propagation framework, due to the graph inductive bias implicit in this process. From this vantage point, we provide explanations for why the learned GNN functions successfully generalize and for their pathological behavior on heterophilic graphs, which are consistent with observations. Practically, sparsifying and implementing the learning dynamics lead to a minimalist semi-supervised learning algorithm with the efficiency of classic algorithms and the effectiveness of modern GNNs.},
	urldate = {2023-10-30},
	publisher = {arXiv},
	author = {Yang, Chenxiao and Wu, Qitian and Wipf, David and Sun, Ruoyu and Yan, Junchi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05105 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
}

@misc{kipfVariationalGraphAutoEncoders2016,
	title = {Variational {Graph} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1611.07308},
	doi = {10.48550/arXiv.1611.07308},
	abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = nov,
	year = {2016},
	note = {arXiv:1611.07308 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@article{olahMechanisticInterpretabilityVariables2022,
	title = {Mechanistic {Interpretability}, {Variables}, and the {Importance} of {Interpretable} {Bases}},
	url = {https://transformer-circuits.pub/2022/mech-interp-essay/},
	journal = {Transformer Circuits Thread},
	author = {Olah, Chris},
	year = {2022},
}

@article{olahFeatureVisualization2017,
	title = {Feature {Visualization}},
	url = {https://distill.pub/2017/feature-visualization},
	doi = {10.23915/distill.00007},
	journal = {Distill},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	year = {2017},
}

@article{johnsonExtensionsLipschitzMappings1984,
	title = {Extensions of {Lipschitz} mappings into {Hilbert} space},
	volume = {26},
	url = {https://stanford.edu/class/cs114/readings/JL-Johnson.pdf},
	journal = {Contemporary mathematics},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	year = {1984},
	pages = {189--206},
}

@article{aroraLinearAlgebraicStructure2018,
	title = {Linear {Algebraic} {Structure} of {Word} {Senses}, with {Applications} to {Polysemy}},
	volume = {6},
	url = {https://doi.org/10.1162/tacl_a_00034},
	doi = {10.1162/tacl\_a\_00034},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	year = {2018},
	keywords = {notion},
	pages = {483--495},
}

@article{heimersheimCircuitPythonDocstrings2023,
	title = {A circuit for {Python} docstrings in a 4-layer attention-only transformer},
	url = {https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only},
	author = {Heimersheim, Stefan and Janiak, Jett},
	year = {2023},
	keywords = {notion},
}

@article{schubertHighLowFrequencyDetectors2021,
	title = {High-{Low} {Frequency} {Detectors}},
	url = {https://distill.pub/2020/circuits/frequency-edges},
	doi = {10.23915/distill.00024.005},
	journal = {Distill},
	author = {Schubert, Ludwig and Voss, Chelsea and Cammarata, Nick and Goh, Gabriel and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{cammarataCurveCircuits2021,
	title = {Curve {Circuits}},
	url = {https://distill.pub/2020/circuits/curve-circuits},
	doi = {10.23915/distill.00024.006},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Voss, Chelsea and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@article{vossBranchSpecialization2021,
	title = {Branch {Specialization}},
	url = {https://distill.pub/2020/circuits/branch-specialization},
	doi = {10.23915/distill.00024.008},
	journal = {Distill},
	author = {Voss, Chelsea and Goh, Gabriel and Cammarata, Nick and Petrov, Michael and Schubert, Ludwig and Olah, Chris},
	year = {2021},
	keywords = {notion},
}

@misc{lindnerTracrCompiledTransformers2023,
	title = {Tracr: {Compiled} {Transformers} as a {Laboratory} for {Interpretability}},
	shorttitle = {Tracr},
	url = {http://arxiv.org/abs/2301.05062},
	abstract = {We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Lindner, David and Kramár, János and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Thomas and Mikulik, Vladimir},
	month = jun,
	year = {2023},
	note = {arXiv:2301.05062 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
}

@inproceedings{geigerCausalAbstractionsNeural2021,
	title = {Causal {Abstractions} of {Neural} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 34: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2021, {NeurIPS} 2021, {December} 6-14, 2021, virtual},
	author = {Geiger, Atticus and Lu, Hanson and Icard, Thomas and Potts, Christopher},
	editor = {Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann N. and Liang, Percy and Vaughan, Jennifer Wortman},
	year = {2021},
	keywords = {notion},
	pages = {9574--9586},
}

